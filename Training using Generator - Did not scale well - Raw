databricks-logox5.1. Training using Generator - Did not scale well - Raw(Python) Import Notebook
Image Processing

Distributed Deep Learning

Get a reference dataframe of location to save model, and parameters of model training
Define function to train model with given parameters and save model to location as specified
Apply .rdd.map to reference dataframe (be sure the partition number is higher than 1)
import numpy as np
import keras
from sklearn.utils import shuffle
import numpy as np
import cv2
def scaling(img,scale_x,scale_y):
    """
    Input an image in a numpy array, scale of the x and ydirection as a decimal
    Outputs a numpy array as the scaled image
    """
    return cv2.resize(img,None,fx=scale_x, fy=scale_y, interpolation = cv2.INTER_LINEAR)
def min_max_normalization(x,min,max):
    """
    This function takes an n by m array and normalizes each value based on the average of min and max values
    of the RBG scale (min=0 and max=255).
    
    It return an n by m array
    """
    avg_value=(max+min)/2.0
    norm_array = np.zeros(x.shape)+avg_value
    normalized_x= (x-norm_array)/norm_array
    return normalized_x
  
def perspectiveTransform(img_src,offset):
    img = img_src
    cols,rows,ch = img.shape
    pts1 = np.float32([[0,0],[rows,0],[rows,cols],[0,cols]])
    pts2 = np.float32([[np.random.randint(low=0-offset,high=offset,size=1),
                        np.random.randint(low=0-offset,high=offset,size=1)],
                       [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                        np.random.randint(low=0-offset,high=offset,size=1)],
                       [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                        np.random.randint(low=cols-offset,high=cols+offset,size=1)],
                       [np.random.randint(low=0-offset,high=offset,size=1),
                        np.random.randint(low=cols-offset,high=cols+offset,size=1)]])

    M = cv2.getPerspectiveTransform(pts1,pts2)

    dst = cv2.warpPerspective(img,M,(rows,cols),borderMode=cv2.BORDER_REPLICATE)
    
    return dst

def rotation(img, rotation_angle):
    rows,cols = img.shape[0:2]

    M = cv2.getRotationMatrix2D((cols/2,rows/2),rotation_angle,1)
    return cv2.warpAffine(img,M,(cols,rows))

def image_read(row):
  img = cv2.imread(row.dst_path)
  flip = row.flip
  aug = row.augmentation
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  if flip == 1:
    img = np.fliplr(img)

  if aug==0:
      img = img
  elif aug==1:
      rotation_angle = np.random.randint(low=-60,high=61,size=1)
      img= rotation(img, rotation_angle)
  elif aug==2:
      aug = np.random.randint(low=0,high=35,size=1)
      img=perspectiveTransform(img,35)
      
  #height_delta = max_height - img.shape[0]
  #width_delta = max_width - img.shape[1]
  return min_max_normalization(img,0,255)

class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, labels, flip, aug, batch_size=1, dim=(200,150), n_channels=3,
                 n_classes=3, shuffle=False):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.labels = labels
        self.list_IDs = list_IDs
        self.n_channels = n_channels
        self.flip = flip
        self.aug = aug
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size), dtype=int)

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            # Store sample
            #print(ID)
            X[i,] = image_read(ID, self.flip[i], self.aug[i])

            # Store class
            y[i] = self.labels[i]

        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)
Using TensorFlow backend.
from PIL import Image 
from pyspark.sql.types import StringType, StructType, DoubleType, StructField,IntegerType
import matplotlib.pyplot as plt

from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
import numpy as np
learning_rate_vector = -4.*np.random.rand(6)
alpha = [10**x for x in learning_rate_vector]
batch_size = np.random.randint(low = 32, high=64, size = 3)
drop_out_a = np.random.uniform(low=0.15, high=0.45, size = 2)
drop_out_b = np.random.uniform(low=0.15, high=0.45, size = 2)
batch_size = np.random.randint(low = 32, high=64, size = 2)
print(alpha)
print(batch_size)
print(drop_out_a)
print(drop_out_b)
import pandas as pd
parameter_list =[]
i=0
for a in alpha:
  for b in batch_size:
    for da in drop_out_a:
      for db in drop_out_b:
        param = {}
        param["Model"] = "/dbfs//Cervix Data/params/paramodel"+str(i)+".h5"
        param["alpha"] = a
        param["batch_size"] = b
        param["drop_out_a"] = da
        param["drop_out_b"] = db
        parameter_list.append(param)
        i+=1
pd_param = pd.DataFrame(parameter_list)
display(pd_param.head())
pd_param.shape
spark_params = spark.createDataFrame(pd_param)
params_path = "dbfs://Cervix Data/old_model/params1.parquet"
models_df_tune = spark.createDataFrame(pd_param)
#models_df_tune.write.parquet(params_path)
dbutils.fs.rm(params_path, recurse=True)
models_df_tune.write.parquet(params_path)
#models_df_tune.write.json("dbfs://Cervix Data/params/params1.json")
dbutils.fs.rm("dbfs://Cervix Data/params/params1.json", recurse=True)
spark_image_data = spark.read.parquet("dbfs://Cervix Data/same_aspect/same_aspect/processed_image_metadata.parquet")
image_data = spark_image_data.toPandas()
image_data.to_json("/dbfs//Cervix Data/params/params1.json")
models_df_tune=spark.read.parquet("dbfs://Cervix Data/params/params1.parquet")
#models_df_tune.cache()
row = models_df_tune.toPandas()
import pandas as pd
image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")
def runDLModelHyper(row):
  import pandas as pd
  #spark_image_data = spark.read.parquet("dbfs://Cervix Data/same_aspect/same_aspect/processed_image_metadata.parquet")
  image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")
  from sklearn.model_selection import train_test_split
  from sklearn.utils import shuffle
  import numpy as np
  from keras.layers.advanced_activations import ELU
  import os
  from keras.models import Sequential
  from keras.layers.core import Dense, Activation, Flatten, Dropout
  from keras.layers.convolutional import Convolution2D
  from keras.layers.pooling import MaxPooling2D
  #from keras.layers.advanced_activations import ELU
  #import pandas as pd
  import datetime
  #import matplotlib.image as mpimg
  import numpy as np
  import time
  import datetime
  import json
  import os
  import tensorflow as tf
  #import horovod.tensorflow
  from tensorflow.contrib import learn
  from keras.datasets import mnist
  #import horovod.tensorflow.keras as hvd
  from tensorflow import keras
  from tensorflow.keras import backend as K
  from tensorflow.keras import layers
  from tensorflow.keras.datasets import mnist
  #from sparkdl import HorovodRunner
  import keras
  #from sklearn.utils import shuffle
  import numpy as np
  import cv2
  from tensorflow.python.client import device_lib
  device_lib.list_local_devices()
  from keras.utils import multi_gpu_model
  from keras import backend as K
  K.tensorflow_backend._get_available_gpus()
  def scaling(img,scale_x,scale_y):
      """
      Input an image in a numpy array, scale of the x and ydirection as a decimal
      Outputs a numpy array as the scaled image
      """
      return cv2.resize(img,None,fx=scale_x, fy=scale_y, interpolation = cv2.INTER_LINEAR)
  def min_max_normalization(x,min,max):
      """
      This function takes an n by m array and normalizes each value based on the average of min and max values
      of the RBG scale (min=0 and max=255).

      It return an n by m array
      """
      avg_value=(max+min)/2.0
      norm_array = np.zeros(x.shape)+avg_value
      normalized_x= (x-norm_array)/norm_array
      return normalized_x

  def perspectiveTransform(img_src,offset):
      img = img_src
      cols,rows,ch = img.shape
      pts1 = np.float32([[0,0],[rows,0],[rows,cols],[0,cols]])
      pts2 = np.float32([[np.random.randint(low=0-offset,high=offset,size=1),
                          np.random.randint(low=0-offset,high=offset,size=1)],
                         [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                          np.random.randint(low=0-offset,high=offset,size=1)],
                         [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                          np.random.randint(low=cols-offset,high=cols+offset,size=1)],
                         [np.random.randint(low=0-offset,high=offset,size=1),
                          np.random.randint(low=cols-offset,high=cols+offset,size=1)]])

      M = cv2.getPerspectiveTransform(pts1,pts2)

      dst = cv2.warpPerspective(img,M,(rows,cols),borderMode=cv2.BORDER_REPLICATE)

      return dst

  def rotation(img, rotation_angle):
      rows,cols = img.shape[0:2]

      M = cv2.getRotationMatrix2D((cols/2,rows/2),rotation_angle,1)
      return cv2.warpAffine(img,M,(cols,rows))

  def image_read(path, flip, aug):
    img = cv2.imread(path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    if flip == 1:
      img = np.fliplr(img)

    if aug==0:
        img = img
    elif aug==1:
        rotation_angle = np.random.randint(low=-60,high=61,size=1)
        img= rotation(img, rotation_angle)
    elif aug==2:
        aug = np.random.randint(low=0,high=35,size=1)
        img=perspectiveTransform(img,35)

    #height_delta = max_height - img.shape[0]
    #width_delta = max_width - img.shape[1]
    return min_max_normalization(img,0,255)

  class DataGenerator(keras.utils.Sequence):
      'Generates data for Keras'
      def __init__(self, list_IDs, labels, flip, aug, batch_size=1, dim=(200,150), n_channels=3,
                   n_classes=3, shuffle=False):
          'Initialization'
          self.dim = dim
          self.batch_size = batch_size
          self.labels = labels
          self.list_IDs = list_IDs
          self.n_channels = n_channels
          self.flip = flip
          self.aug = aug
          self.n_classes = n_classes
          self.shuffle = shuffle
          self.on_epoch_end()

      def __len__(self):
          'Denotes the number of batches per epoch'
          return int(np.floor(len(self.list_IDs) / self.batch_size))

      def __getitem__(self, index):
          'Generate one batch of data'
          # Generate indexes of the batch
          indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

          # Find list of IDs
          list_IDs_temp = [self.list_IDs[k] for k in indexes]

          # Generate data
          X, y = self.__data_generation(list_IDs_temp)

          return X, y

      def on_epoch_end(self):
          'Updates indexes after each epoch'
          self.indexes = np.arange(len(self.list_IDs))
          if self.shuffle == True:
              np.random.shuffle(self.indexes)

      def __data_generation(self, list_IDs_temp):
          'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
          # Initialization
          X = np.empty((self.batch_size, *self.dim, self.n_channels))
          y = np.empty((self.batch_size), dtype=int)

          # Generate data
          for i, ID in enumerate(list_IDs_temp):
              # Store sample
              #print(ID)
              X[i,] = image_read(ID, self.flip[i], self.aug[i])

              # Store class
              y[i] = self.labels[i]

          return X, keras.utils.to_categorical(y, num_classes=self.n_classes)
  print(row.Model)
  train, valid = train_test_split(image_data, test_size=0.1)

  train_reg = train.copy()
  train_reg["augmentation"] = 0

  train_rot = train.copy()
  train_rot["augmentation"] = 1

  train_warp = train.copy()
  train_warp["augmentation"] = 2

  train_aug = pd.concat([train_reg,train_rot,train_warp])

  train_flip = train_aug.copy()
  train_flip["flip"] = 1

  train_nonflip = train_aug.copy()
  train_nonflip["flip"] = 0

  train_final = shuffle(pd.concat([train_nonflip,train_flip]))
  valid["flip"] = 0
  valid["augmentation"] = 0
  train_generator = DataGenerator(train_final.dst_path.tolist(), train_final.label.tolist(), train_final.flip.tolist(), train_final.augmentation.tolist(), batch_size=int(row.batch_size))
  val_generator = DataGenerator(valid.dst_path.tolist(), valid.label.tolist(), valid.flip.tolist(), valid.augmentation.tolist(),batch_size=40)
  
  model = Sequential()
  elu=ELU()
  #model.add(Convolution2D(5, 5, 5, batch_input_shape=(None, 450,600,3)))
  model.add(Convolution2D(64, (3, 3), batch_input_shape=(None, 200,150,3)))
  model.add(elu)
  model.add(MaxPooling2D((2, 2)))

  model.add(Convolution2D(16, (3, 3)))
  model.add(elu)

  model.add(Convolution2D(32, (3, 3)))
  model.add(elu)
  model.add(MaxPooling2D((1, 2)))


  model.add(Convolution2D(64, (3, 3)))
  model.add(elu)
  model.add(MaxPooling2D((2, 1)))


  model.add(Convolution2D(128, (3, 3)))
  model.add(MaxPooling2D((2, 2)))
  model.add(elu)

  model.add(Convolution2D(256, (3, 3)))
  model.add(elu)
  model.add(MaxPooling2D((2, 2)))


  model.add(Convolution2D(512, (3, 3)))
  model.add(elu)
  model.add(MaxPooling2D((2, 2)))



  model.add(Flatten())

  #model.add(Dense(10000))
  #model.add(Activation('relu'))
  #model.add(Dropout(0.2))

  #model.add(Dense(2500))
  #model.add(Activation('relu'))
  #model.add(Dropout(0.2))


  model.add(Dense(1000))
  model.add(Activation('relu'))
  model.add(Dropout(row.drop_out_a))

  #model.add(Dense(800))
  #model.add(Activation('relu'))
  #model.add(Dropout(0.2))

  model.add(Dense(700))
  model.add(Activation('relu'))
  model.add(Dropout(row.drop_out_a))

  #model.add(Dense(600))
  #model.add(Activation('relu'))
  #model.add(Dropout(0.2))

  #model.add(Dense(500))
  #model.add(Activation('relu'))
  #model.add(Dropout(0.2))

  model.add(Dense(400))
  model.add(Activation('relu'))
  model.add(Dropout(row.drop_out_a))

  model.add(Dense(300))
  model.add(Activation('relu'))
  model.add(Dropout(row.drop_out_a))

  model.add(Dense(200))
  model.add(Activation('relu'))
  model.add(Dropout(row.drop_out_b))

  model.add(Dense(100))
  model.add(Activation('relu'))
  model.add(Dropout(row.drop_out_b))

  model.add(Dense(50))
  model.add(Activation('relu'))
  model.add(Dropout(row.drop_out_b))

  """model.add(Dense(40))
  model.add(Activation('relu'))
  model.add(Dropout(0.1))

  model.add(Dense(30))
  model.add(Activation('relu'))
  model.add(Dropout(0.1))

  model.add(Dense(25))
  model.add(Activation('relu'))
  model.add(Dropout(0.1))

  model.add(Dense(20))
  model.add(Activation('relu'))
  #model.add(Dropout(0.2))"""

  model.add(Dense(15))
  model.add(Activation('relu'))
  #model.add(Dropout(0.2))

  #model.add(Dense(5))
  #model.add(Activation('relu'))


  model.add(Dense(3))
  model.add(Activation('softmax'))

  model.summary()

  model.compile(loss=keras.losses.categorical_crossentropy,
                optimizer=keras.optimizers.Adam(lr = row.alpha),
                metrics=['accuracy'])
  #batch_size = 100
  from sklearn.utils import class_weight

  class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(train_final.label),
                                                 train_final.label)
  epochs = 1

  model.fit_generator(train_generator, validation_data=val_generator, epochs=epochs, max_queue_size=10,  workers=10, class_weight = class_weights, use_multiprocessing=True, verbose=0)
  model.save(str(row.Model))
  
  score = model.evaluate_generator(val_generator, max_queue_size=10,workers=10, use_multiprocessing=True, verbose=0)
  acc = float(score[1])
  loss = float(score[0])
  #acc = 1.0
  #loss = 1.0
  return(row.Model, row.alpha, row.batch_size, row.drop_out_a, row.drop_out_b, loss, acc)
schema = StructType([
  StructField("ModelLocation", StringType(), False),
  StructField("Alpha", DoubleType(), False),
  StructField("BatchSize", IntegerType(), False),
  StructField("DropoutA", DoubleType(), False),
  StructField("DropoutB", DoubleType(), False),
  StructField("Loss", DoubleType(), False),
  StructField("Accuracy", DoubleType(), False)
                    ])
models_df_tune = models_df_tune.repartition(72)
models_df_tune.cache()
Out[6]: DataFrame[Model: string, alpha: double, batch_size: bigint, drop_out_a: double, drop_out_b: double]
results_df = models_df_tune.rdd.map(runDLModelHyper).toDF(schema)
# Spark having lazy evaluation, the <display> action actually 'runs' the compute
display(results_df)
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_test.shape
row.iloc[0:2]
runDLModelHyper(row.iloc[0])
image_read("/dbfs//Cervix Data/same_aspect/Type_2/115.jpg",0,0).shape

display(spark_image_data)
image_data = spark_image_data.toPandas()
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import pandas as pd
train, valid = train_test_split(image_data, test_size=0.1)

train_reg = train.copy()
train_reg["augmentation"] = 0

train_rot = train.copy()
train_rot["augmentation"] = 1

train_warp = train.copy()
train_warp["augmentation"] = 2

train_aug = pd.concat([train_reg,train_rot,train_warp])

train_flip = train_aug.copy()
train_flip["flip"] = 1

train_nonflip = train_aug.copy()
train_nonflip["flip"] = 0

train_final = shuffle(pd.concat([train_nonflip,train_flip]),random_state =100)
valid["flip"] = 0
valid["augmentation"] = 0
spark_train_data = spark.createDataFrame(train_final)
from pyspark.sql.types import ArrayType, DoubleType
array = StructType([StructField("image",ArrayType([
  ArrayType(DoubleType()),
  ArrayType(DoubleType()),
  ArrayType(DoubleType())
                    ]))])
def resultDLModelHyper(row):
  import pandas as pd
  #spark_image_data = spark.read.parquet("dbfs://Cervix Data/same_aspect/same_aspect/processed_image_metadata.parquet")
  image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")
  from sklearn.model_selection import train_test_split
  from sklearn.utils import shuffle
  import numpy as np
  from keras.layers.advanced_activations import ELU
  import os
  from keras.models import Sequential
  from keras.layers.core import Dense, Activation, Flatten, Dropout
  from keras.layers.convolutional import Convolution2D
  from keras.layers.pooling import MaxPooling2D
  #from keras.layers.advanced_activations import ELU
  #import pandas as pd
  import datetime
  #import matplotlib.image as mpimg
  import numpy as np
  import time
  import datetime
  import json
  import os
  import tensorflow as tf
  #import horovod.tensorflow
  from tensorflow.contrib import learn
  from keras.datasets import mnist
  #import horovod.tensorflow.keras as hvd
  from tensorflow import keras
  from tensorflow.keras import backend as K
  from tensorflow.keras import layers
  from tensorflow.keras.datasets import mnist
  #from sparkdl import HorovodRunner
  import keras
  #from sklearn.utils import shuffle
  import numpy as np
  import cv2
  from tensorflow.python.client import device_lib
  device_lib.list_local_devices()
  from keras.utils import multi_gpu_model
  from keras import backend as K
  K.tensorflow_backend._get_available_gpus()
  def scaling(img,scale_x,scale_y):
      """
      Input an image in a numpy array, scale of the x and ydirection as a decimal
      Outputs a numpy array as the scaled image
      """
      return cv2.resize(img,None,fx=scale_x, fy=scale_y, interpolation = cv2.INTER_LINEAR)
  def min_max_normalization(x,min,max):
      """
      This function takes an n by m array and normalizes each value based on the average of min and max values
      of the RBG scale (min=0 and max=255).

      It return an n by m array
      """
      avg_value=(max+min)/2.0
      norm_array = np.zeros(x.shape)+avg_value
      normalized_x= (x-norm_array)/norm_array
      return normalized_x

  def perspectiveTransform(img_src,offset):
      img = img_src
      cols,rows,ch = img.shape
      pts1 = np.float32([[0,0],[rows,0],[rows,cols],[0,cols]])
      pts2 = np.float32([[np.random.randint(low=0-offset,high=offset,size=1),
                          np.random.randint(low=0-offset,high=offset,size=1)],
                         [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                          np.random.randint(low=0-offset,high=offset,size=1)],
                         [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                          np.random.randint(low=cols-offset,high=cols+offset,size=1)],
                         [np.random.randint(low=0-offset,high=offset,size=1),
                          np.random.randint(low=cols-offset,high=cols+offset,size=1)]])

      M = cv2.getPerspectiveTransform(pts1,pts2)

      dst = cv2.warpPerspective(img,M,(rows,cols),borderMode=cv2.BORDER_REPLICATE)

      return dst

  def rotation(img, rotation_angle):
      rows,cols = img.shape[0:2]

      M = cv2.getRotationMatrix2D((cols/2,rows/2),rotation_angle,1)
      return cv2.warpAffine(img,M,(cols,rows))

  def image_read(path, flip, aug):
    img = cv2.imread(path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    if flip == 1:
      img = np.fliplr(img)

    if aug==0:
        img = img
    elif aug==1:
        rotation_angle = np.random.randint(low=-60,high=61,size=1)
        img= rotation(img, rotation_angle)
    elif aug==2:
        aug = np.random.randint(low=0,high=35,size=1)
        img=perspectiveTransform(img,35)

    #height_delta = max_height - img.shape[0]
    #width_delta = max_width - img.shape[1]
    return min_max_normalization(img,0,255)

  class DataGenerator(keras.utils.Sequence):
      'Generates data for Keras'
      def __init__(self, list_IDs, labels, flip, aug, batch_size=1, dim=(200,150), n_channels=3,
                   n_classes=3, shuffle=False):
          'Initialization'
          self.dim = dim
          self.batch_size = batch_size
          self.labels = labels
          self.list_IDs = list_IDs
          self.n_channels = n_channels
          self.flip = flip
          self.aug = aug
          self.n_classes = n_classes
          self.shuffle = shuffle
          self.on_epoch_end()

      def __len__(self):
          'Denotes the number of batches per epoch'
          return int(np.floor(len(self.list_IDs) / self.batch_size))

      def __getitem__(self, index):
          'Generate one batch of data'
          # Generate indexes of the batch
          indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

          # Find list of IDs
          list_IDs_temp = [self.list_IDs[k] for k in indexes]

          # Generate data
          X, y = self.__data_generation(list_IDs_temp)

          return X, y

      def on_epoch_end(self):
          'Updates indexes after each epoch'
          self.indexes = np.arange(len(self.list_IDs))
          if self.shuffle == True:
              np.random.shuffle(self.indexes)

      def __data_generation(self, list_IDs_temp):
          'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
          # Initialization
          X = np.empty((self.batch_size, *self.dim, self.n_channels))
          y = np.empty((self.batch_size), dtype=int)

          # Generate data
          for i, ID in enumerate(list_IDs_temp):
              # Store sample
              #print(ID)
              X[i,] = image_read(ID, self.flip[i], self.aug[i])

              # Store class
              y[i] = self.labels[i]

          return X, keras.utils.to_categorical(y, num_classes=self.n_classes)
  print(row.Model)
  train, valid = train_test_split(image_data, test_size=0.1)

  train_reg = train.copy()
  train_reg["augmentation"] = 0

  train_rot = train.copy()
  train_rot["augmentation"] = 1

  train_warp = train.copy()
  train_warp["augmentation"] = 2

  train_aug = pd.concat([train_reg,train_rot,train_warp])

  train_flip = train_aug.copy()
  train_flip["flip"] = 1

  train_nonflip = train_aug.copy()
  train_nonflip["flip"] = 0

  train_final = shuffle(pd.concat([train_nonflip,train_flip]))
  valid["flip"] = 0
  valid["augmentation"] = 0
  #train_generator = DataGenerator(train_final.dst_path.tolist(), train_final.label.tolist(), train_final.flip.tolist(), train_final.augmentation.tolist(), batch_size=int(row.batch_size))
  val_generator = DataGenerator(valid.dst_path.tolist(), valid.label.tolist(), valid.flip.tolist(), valid.augmentation.tolist(),batch_size=40)
  
  from keras.models import load_model
  model = load_model(str(row.Model))
  score = model.evaluate_generator(val_generator, max_queue_size=10,workers=10, use_multiprocessing=True, verbose=0)
  acc = float(score[1])
  loss = float(score[0])
  #acc = 1.0
  #loss = 1.0
  return(row.Model, row.alpha, row.batch_size, row.drop_out_a, row.drop_out_b, loss, acc)
model_results_df = models_df_tune.rdd.map(runDLModelHyper).toDF(schema)
display(model_results_df)
/dbfs/pepar/Cervix Data/params/paramodel30.h5	0.5513165357378691	40	0.16474441403808845	0.39799962590853877	8.461999893188477	0.4749999940395355
/dbfs/pepar/Cervix Data/params/paramodel31.h5	0.5513165357378691	40	0.16474441403808845	0.28236743026273065	9.267904281616211	0.42500001192092896
/dbfs/pepar/Cervix Data/params/paramodel38.h5	0.034861897549204025	40	0.16474441403808845	0.39799962590853877	7.253143310546875	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel36.h5	0.034861897549204025	40	0.4185162666207506	0.39799962590853877	11.28266716003418	0.30000001192092896
/dbfs/pepar/Cervix Data/params/paramodel37.h5	0.034861897549204025	40	0.4185162666207506	0.28236743026273065	8.461999893188477	0.4749999940395355
/dbfs/pepar/Cervix Data/params/paramodel39.h5	0.034861897549204025	40	0.16474441403808845	0.28236743026273065	5.64133358001709	0.6499999761581421
/dbfs/pepar/Cervix Data/params/paramodel44.h5	0.0001861157898397383	40	0.4185162666207506	0.39799962590853877	0.9909324771479556	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel47.h5	0.0001861157898397383	40	0.16474441403808845	0.28236743026273065	1.0945268430207904	0.42500001192092896
/dbfs/pepar/Cervix Data/params/paramodel46.h5	0.0001861157898397383	40	0.16474441403808845	0.39799962590853877	0.9883882121035927	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel45.h5	0.0001861157898397383	40	0.4185162666207506	0.28236743026273065	1.0007076106573407	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel12.h5	0.004278005856800815	40	0.4185162666207506	0.39799962590853877	11.28266716003418	0.30000001192092896
/dbfs/pepar/Cervix Data/params/paramodel13.h5	0.004278005856800815	40	0.4185162666207506	0.28236743026273065	1.011622667312622	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel14.h5	0.004278005856800815	40	0.16474441403808845	0.39799962590853877	0.9776795506477356	0.6499999761581421
/dbfs/pepar/Cervix Data/params/paramodel15.h5	0.004278005856800815	40	0.16474441403808845	0.28236743026273065	1.019781470298767	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel21.h5	0.10121664423881171	40	0.4185162666207506	0.28236743026273065	8.461999893188477	0.4749999940395355
/dbfs/pepar/Cervix Data/params/paramodel22.h5	0.10121664423881171	40	0.16474441403808845	0.39799962590853877	9.267904281616211	0.42500001192092896
/dbfs/pepar/Cervix Data/params/paramodel20.h5	0.10121664423881171	40	0.4185162666207506	0.39799962590853877	7.253143310546875	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel23.h5	0.10121664423881171	40	0.16474441403808845	0.28236743026273065	7.253143310546875	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel5.h5	0.0002163529442559525	40	0.4185162666207506	0.28236743026273065	0.9836059620505885	0.4605263142209304
/dbfs/pepar/Cervix Data/params/paramodel4.h5	0.0002163529442559525	40	0.4185162666207506	0.39799962590853877	1.0563214640868337	0.4749999940395355
/dbfs/pepar/Cervix Data/params/paramodel7.h5	0.0002163529442559525	40	0.16474441403808845	0.28236743026273065	0.9949276572779605	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel6.h5	0.0002163529442559525	40	0.16474441403808845	0.39799962590853877	1.0092112202393382	0.550000011920929
/dbfs/pepar/Cervix Data/params/paramodel29.h5	0.5513165357378691	40	0.4185162666207506	0.28236743026273065	5.64133358001709	0.6499999761581421
/dbfs/pepar/Cervix Data/params/paramodel28.h5	0.5513165357378691	40	0.4185162666207506	0.39799962590853877	7.253143310546875	0.550000011920929
ModelLocation	Alpha	BatchSize	DropoutA	DropoutB	Loss	Accuracy
 pd_results_df = models_df_tune.rdd.map(resultDLModelHyper).toDF(schema).toPandas()
resultDLModelHyper(row.iloc[0])
/dbfs/pepar/Cervix Data/params/paramodel30.h5
/local_disk0/tmp/1551191842688-0/PythonShell.py:180: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self._dropped += pos
/local_disk0/tmp/1551191842688-0/PythonShell.py:181: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  else:
Out[18]: 
('/dbfs/pepar/Cervix Data/params/paramodel30.h5',
 0.5513165357378691,
 40,
 0.16474441403808845,
 0.39799962590853877,
 7.253143310546875,
 0.550000011920929)
pd_results_df
Out[20]: 
                                    ModelLocation    ...     Accuracy
0   /dbfs/pepar/Cervix Data/params/paramodel30.h5    ...     0.475000
1   /dbfs/pepar/Cervix Data/params/paramodel31.h5    ...     0.425000
2   /dbfs/pepar/Cervix Data/params/paramodel38.h5    ...     0.550000
3   /dbfs/pepar/Cervix Data/params/paramodel36.h5    ...     0.300000
4   /dbfs/pepar/Cervix Data/params/paramodel37.h5    ...     0.475000
5   /dbfs/pepar/Cervix Data/params/paramodel39.h5    ...     0.650000
6   /dbfs/pepar/Cervix Data/params/paramodel44.h5    ...     0.550000
7   /dbfs/pepar/Cervix Data/params/paramodel47.h5    ...     0.425000
8   /dbfs/pepar/Cervix Data/params/paramodel46.h5    ...     0.550000
9   /dbfs/pepar/Cervix Data/params/paramodel45.h5    ...     0.550000
10  /dbfs/pepar/Cervix Data/params/paramodel12.h5    ...     0.300000
11  /dbfs/pepar/Cervix Data/params/paramodel13.h5    ...     0.550000
12  /dbfs/pepar/Cervix Data/params/paramodel14.h5    ...     0.650000
13  /dbfs/pepar/Cervix Data/params/paramodel15.h5    ...     0.550000
14  /dbfs/pepar/Cervix Data/params/paramodel21.h5    ...     0.475000
15  /dbfs/pepar/Cervix Data/params/paramodel22.h5    ...     0.425000
16  /dbfs/pepar/Cervix Data/params/paramodel20.h5    ...     0.550000
17  /dbfs/pepar/Cervix Data/params/paramodel23.h5    ...     0.550000
18   /dbfs/pepar/Cervix Data/params/paramodel5.h5    ...     0.467105
19   /dbfs/pepar/Cervix Data/params/paramodel4.h5    ...     0.550000
20   /dbfs/pepar/Cervix Data/params/paramodel7.h5    ...     0.550000
21   /dbfs/pepar/Cervix Data/params/paramodel6.h5    ...     0.550000
22  /dbfs/pepar/Cervix Data/params/paramodel29.h5    ...     0.650000
23  /dbfs/pepar/Cervix Data/params/paramodel28.h5    ...     0.550000

[24 rows x 7 columns]
