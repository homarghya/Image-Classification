databricks-logox5.3. Training using Reading All Photos on Worker - Did not scale well - Raw(Python) Import Notebook
Image Processing

Distributed Deep Learning

Get a reference dataframe of location to save model, and parameters of model training
Define function to train model with given parameters and save model to location as specified
Apply .rdd.map to reference dataframe (be sure the partition number is higher than 1)
import pandas as pd
image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")
image_data.shape
Cancelled
ignore_wrong_labal = [x not in ["/dbfs//Cervix Data/same_aspect/Type_1/80.jpg", "/dbfs//Cervix Data/same_aspect/Type_3/968.jpg","/dbfs//Cervix Data/same_aspect/Type_3/1120.jpg"] for x in image_data["dst_path"]]
image_data = image_data[ignore_wrong_labal]
image_data.shape
Cancelled
import numpy as np
import keras
from sklearn.utils import shuffle
import numpy as np
import cv2
Using TensorFlow backend.
from PIL import Image 
from pyspark.sql.types import StringType, StructType, DoubleType, StructField,IntegerType
import matplotlib.pyplot as plt

from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
import pandas as pd
models_df_tune=spark.read.parquet("dbfs://Cervix Data/params/params1_v10.parquet")
#models_df_tune.cache()
models_df_tune = models_df_tune.filter(models_df_tune.Loss < 2.1)
row = models_df_tune.toPandas()
row.head()
Out[8]: 
                                       ModelLocation    ...     Accuracy
0  /dbfs/pepar/Cervix Data/params/paramodel44_v10.h5    ...        0.575
1   /dbfs/pepar/Cervix Data/params/paramodel6_v10.h5    ...        0.575
2  /dbfs/pepar/Cervix Data/params/paramodel13_v10.h5    ...        0.575
3   /dbfs/pepar/Cervix Data/params/paramodel5_v10.h5    ...        0.575
4  /dbfs/pepar/Cervix Data/params/paramodel46_v10.h5    ...        0.575

[5 rows x 7 columns]
display(row)
/dbfs/pepar/Cervix Data/params/paramodel44_v10.h5	0.0001861157898397383	40	0.4185162666207506	0.39799962590853877	0.9870204549086722	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel6_v10.h5	0.0002163529442559525	40	0.16474441403808845	0.39799962590853877	0.9806448879994845	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel13_v10.h5	0.004278005856800815	40	0.4185162666207506	0.28236743026273065	0.9666332006454468	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel5_v10.h5	0.0002163529442559525	40	0.4185162666207506	0.28236743026273065	0.9651263575804861	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel46_v10.h5	0.0001861157898397383	40	0.16474441403808845	0.39799962590853877	0.999258838201824	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel45_v10.h5	0.0001861157898397383	40	0.4185162666207506	0.28236743026273065	0.9638148577589738	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel15_v10.h5	0.004278005856800815	40	0.16474441403808845	0.28236743026273065	0.9904924631118774	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel4_v10.h5	0.0002163529442559525	40	0.4185162666207506	0.39799962590853877	1.0187324850182784	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel14_v10.h5	0.004278005856800815	40	0.16474441403808845	0.39799962590853877	0.9604905247688293	0.574999988079071
/dbfs/pepar/Cervix Data/params/paramodel7_v10.h5	0.0002163529442559525	40	0.16474441403808845	0.28236743026273065	0.9708177666915091	0.5736841998602215
ModelLocation	Alpha	BatchSize	DropoutA	DropoutB	Loss	Accuracy
 models_df_tune.count()
Out[9]: 10
str(row.iloc[18].ModelLocation)[:-6]+"_v3.h5"
Cancelled
def runDLModelHyper(row):
  import pandas as pd
  #spark_image_data = spark.read.parquet("dbfs://Cervix Data/same_aspect/same_aspect/processed_image_metadata.parquet")
  image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")
  from sklearn.model_selection import train_test_split
  from sklearn.utils import shuffle
  import numpy as np
  from keras.layers.advanced_activations import ELU
  import os
  from keras.models import Sequential
  from keras.layers.core import Dense, Activation, Flatten, Dropout
  from keras.layers.convolutional import Convolution2D
  from keras.layers.pooling import MaxPooling2D
  #from keras.layers.advanced_activations import ELU
  #import pandas as pd
  import datetime
  #import matplotlib.image as mpimg
  import numpy as np
  import time
  import datetime
  import json
  import os
  import tensorflow as tf
  #import horovod.tensorflow
  from tensorflow.contrib import learn
  from keras.datasets import mnist
  #import horovod.tensorflow.keras as hvd
  from tensorflow import keras
  from tensorflow.keras import backend as K
  from tensorflow.keras import layers
  from tensorflow.keras.datasets import mnist
  #from sparkdl import HorovodRunner
  import keras
  #from sklearn.utils import shuffle
  import numpy as np
  import cv2
  from tensorflow.python.client import device_lib
  device_lib.list_local_devices()
  from keras.utils import multi_gpu_model
  from keras import backend as K
  K.tensorflow_backend._get_available_gpus()
  
  def min_max_normalization(x,min,max):
    """
    This function takes an n by m array and normalizes each value based on the average of min and max values
    of the RBG scale (min=0 and max=255).

    It return an n by m array
    """
    avg_value=(max+min)/2.0
    norm_array = np.zeros(x.shape)+avg_value
    normalized_x= (x-norm_array)/norm_array
    return normalized_x
  
  train, valid = train_test_split(image_data, test_size=0.1)
  
  images = []
  angles = []
  print("augmenting Data")
  for batch_sample, label in zip(sc_X_train.value,sc_y_train.value):
    
    #name = batch_sample.dst_path
    img = batch_sample
    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    rotation_angle = np.random.randint(low=-60,high=61,size=1)
    img_rot= min_max_normalization(rotation(img, rotation_angle), 0, 255)
    aug = np.random.randint(low=1,high=20,size=1)[0]
    #img_aug=min_max_normalization(perspectiveTransform(img,aug), 0, 255)
    img_flip = min_max_normalization(np.fliplr(img), 0, 255)
    rotation_angle = np.random.randint(low=-60,high=61,size=1)
    img_flip_rot= min_max_normalization(rotation(img_flip, rotation_angle), 0, 255)
    aug = np.random.randint(low=1,high=20,size=1)[0]
    #img_flip_aug=min_max_normalization(perspectiveTransform(img_flip,aug), 0, 255) 
    img = min_max_normalization(img, 0, 255)
    #center_image = image_read(name, batch_sample.flip, batch_sample.augmentation)
    center_angle = label
    #center_angle[batch_sample.label] = 1
    images.append(img)
    images.append(img_rot)
    #images.append(img_aug)
    images.append(img_flip)
    images.append(img_flip_rot)
    #images.append(img_flip_aug)
    angles.append(center_angle)
    angles.append(center_angle)
    angles.append(center_angle)
    #angles.append(center_angle)
    #angles.append(center_angle)
    angles.append(center_angle)
    #images.append(np.fliplr(center_image))
    #angles.append([1-center_angle,center_angle])
    
  X_train = np.array(images)
  y_train = np.array(angles)
  X_train, y_train = shuffle(X_train, y_train)
  from keras.models import load_model
  model = load_model(str(row.ModelLocation)[:-7]+"_v11.h5")

  model.compile(loss=keras.losses.categorical_crossentropy,
                optimizer=keras.optimizers.Adam(lr = row.Alpha),
                metrics=['accuracy'])
  #batch_size = 100
  from sklearn.utils import class_weight

  class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(train.label),
                                                 train.label)
  epochs = 30

  #model.fit(X_train, y_train, batch_size = 100, epochs=epochs, class_weight = class_weights, verbose=1)
  model.fit(X_train, y_train, batch_size = 100, epochs=epochs, class_weight = class_weights, verbose=1)
  model.save(str(row.ModelLocation)[:-7]+"_v12.h5")
  
  score = model.evaluate(sc_X_val.value, sc_y_val.value,verbose=0)
  acc = float(score[1])
  loss = float(score[0])
  #acc = 1.0
  #loss = 1.0
  return(str(row.ModelLocation)[:-7]+"_v12.h5", row.Alpha, row.BatchSize, row.DropoutA, row.DropoutB, loss, acc)
schema = StructType([
  StructField("ModelLocation", StringType(), False),
  StructField("Alpha", DoubleType(), False),
  StructField("BatchSize", IntegerType(), False),
  StructField("DropoutA", DoubleType(), False),
  StructField("DropoutB", DoubleType(), False),
  StructField("Loss", DoubleType(), False),
  StructField("Accuracy", DoubleType(), False)
                    ])
models_df_tune = models_df_tune.repartition(10)
models_df_tune.cache()
Out[15]: DataFrame[ModelLocation: string, Alpha: double, BatchSize: bigint, DropoutA: double, DropoutB: double, Loss: double, Accuracy: double]
results_df = models_df_tune.rdd.map(runDLModelHyper).toDF(schema).toPandas()
Cancelled
# Spark having lazy evaluation, the <display> action actually 'runs' the compute
display(results_df)
NameError: name 'results_df' is not defined
#dbutils.fs.rm("dbfs://Cervix Data/params/params1_v7.parquet", recurse=True)
spark.createDataFrame(results_df).write.parquet("dbfs://Cervix Data/params/params1_v11.parquet")
Cancelled
(x_train, y_train), (x_test, y_test) = mnist.load_data()
Cancelled
x_test.shape
Cancelled
row.iloc[8]
Out[19]: 
ModelLocation    /dbfs/pepar/Cervix Data/params/paramodel14_v10.h5
Alpha                                                   0.00427801
BatchSize                                                       40
DropoutA                                                  0.164744
DropoutB                                                     0.398
Loss                                                      0.960491
Accuracy                                                     0.575
Name: 8, dtype: object
runDLModelHyper(row.iloc[8])
augmenting Data
Epoch 1/30

  100/27456 [..............................] - ETA: 9:13 - loss: 1.0307 - acc: 0.5000
  200/27456 [..............................] - ETA: 4:47 - loss: 1.0229 - acc: 0.5000
  300/27456 [..............................] - ETA: 3:18 - loss: 1.0169 - acc: 0.5067
  400/27456 [..............................] - ETA: 2:34 - loss: 1.0116 - acc: 0.5200
  500/27456 [..............................] - ETA: 2:07 - loss: 1.0103 - acc: 0.5200
  600/27456 [..............................] - ETA: 1:49 - loss: 1.0110 - acc: 0.5167
  700/27456 [..............................] - ETA: 1:36 - loss: 1.0102 - acc: 0.5214
  800/27456 [..............................] - ETA: 1:27 - loss: 1.0108 - acc: 0.5187
  900/27456 [..............................] - ETA: 1:19 - loss: 1.0108 - acc: 0.5200
 1000/27456 [>.............................] - ETA: 1:13 - loss: 1.0190 - acc: 0.5100
 1100/27456 [>.............................] - ETA: 1:08 - loss: 1.0144 - acc: 0.5155
 1200/27456 [>.............................] - ETA: 1:04 - loss: 1.0084 - acc: 0.5217
 1300/27456 [>.............................] - ETA: 1:00 - loss: 1.0212 - acc: 0.5231
 1400/27456 [>.............................] - ETA: 57s - loss: 1.0260 - acc: 0.5171 
 1500/27456 [>.............................] - ETA: 55s - loss: 1.0301 - acc: 0.5093
 1600/27456 [>.............................] - ETA: 53s - loss: 1.0314 - acc: 0.5087
 1700/27456 [>.............................] - ETA: 50s - loss: 1.0265 - acc: 0.5141
 1800/27456 [>.............................] - ETA: 49s - loss: 1.0256 - acc: 0.5128
 1900/27456 [=>............................] - ETA: 47s - loss: 1.0278 - acc: 0.5105
 2000/27456 [=>............................] - ETA: 46s - loss: 1.0249 - acc: 0.5125
 2100/27456 [=>............................] - ETA: 44s - loss: 1.0283 - acc: 0.5076
 2200/27456 [=>............................] - ETA: 43s - loss: 1.0301 - acc: 0.5059
 2300/27456 [=>............................] - ETA: 42s - loss: 1.0270 - acc: 0.5078
 2400/27456 [=>............................] - ETA: 41s - loss: 1.0248 - acc: 0.5100
 2500/27456 [=>............................] - ETA: 40s - loss: 1.0205 - acc: 0.5140
 2600/27456 [=>............................] - ETA: 39s - loss: 1.0197 - acc: 0.5146
 2700/27456 [=>............................] - ETA: 38s - loss: 1.0274 - acc: 0.5115
 2800/27456 [==>...........................] - ETA: 37s - loss: 1.0302 - acc: 0.5079
 2900/27456 [==>...........................] - ETA: 36s - loss: 1.0308 - acc: 0.5069
 3000/27456 [==>...........................] - ETA: 36s - loss: 1.0320 - acc: 0.5040
 3100/27456 [==>...........................] - ETA: 35s - loss: 1.0300 - acc: 0.5065
 3200/27456 [==>...........................] - ETA: 34s - loss: 1.0324 - acc: 0.5047
 3300/27456 [==>...........................] - ETA: 34s - loss: 1.0301 - acc: 0.5064
 3400/27456 [==>...........................] - ETA: 33s - loss: 1.0287 - acc: 0.5074
 3500/27456 [==>...........................] - ETA: 33s - loss: 1.0277 - acc: 0.5074
 3600/27456 [==>...........................] - ETA: 32s - loss: 1.0276 - acc: 0.5072
 3700/27456 [===>..........................] - ETA: 32s - loss: 1.0281 - acc: 0.5068
 3800/27456 [===>..........................] - ETA: 31s - loss: 1.0290 - acc: 0.5058
 3900/27456 [===>..........................] - ETA: 31s - loss: 1.0281 - acc: 0.5062
 4000/27456 [===>..........................] - ETA: 30s - loss: 1.0268 - acc: 0.5070
 4100/27456 [===>..........................] - ETA: 30s - loss: 1.0259 - acc: 0.5083
 4200/27456 [===>..........................] - ETA: 30s - loss: 1.0260 - acc: 0.5076
 4300/27456 [===>..........................] - ETA: 29s - loss: 1.0261 - acc: 0.5074
 4400/27456 [===>..........................] - ETA: 29s - loss: 1.0250 - acc: 0.5080
 4500/27456 [===>..........................] - ETA: 29s - loss: 1.0258 - acc: 0.5071
 4600/27456 [====>.........................] - ETA: 28s - loss: 1.0266 - acc: 0.5059
 4700/27456 [====>.........................] - ETA: 28s - loss: 1.0262 - acc: 0.5064
 4800/27456 [====>.........................] - ETA: 28s - loss: 1.0257 - acc: 0.5069
 4900/27456 [====>.........................] - ETA: 27s - loss: 1.0246 - acc: 0.5069
 5000/27456 [====>.........................] - ETA: 27s - loss: 1.0251 - acc: 0.5066
 5100/27456 [====>.........................] - ETA: 27s - loss: 1.0260 - acc: 0.5053
 5200/27456 [====>.........................] - ETA: 26s - loss: 1.0273 - acc: 0.5033
 5300/27456 [====>.........................] - ETA: 26s - loss: 1.0275 - acc: 0.5032
 5400/27456 [====>.........................] - ETA: 26s - loss: 1.0278 - acc: 0.5024
 5500/27456 [=====>........................] - ETA: 26s - loss: 1.0304 - acc: 0.5027
 5600/27456 [=====>........................] - ETA: 25s - loss: 1.0296 - acc: 0.5037
 5700/27456 [=====>........................] - ETA: 25s - loss: 1.0299 - acc: 0.5030
 5800/27456 [=====>........................] - ETA: 25s - loss: 1.0295 - acc: 0.5031
 5900/27456 [=====>........................] - ETA: 25s - loss: 1.0304 - acc: 0.5015
 6000/27456 [=====>........................] - ETA: 24s - loss: 1.0301 - acc: 0.5022
 6100/27456 [=====>........................] - ETA: 24s - loss: 1.0302 - acc: 0.5018
 6200/27456 [=====>........................] - ETA: 24s - loss: 1.0308 - acc: 0.5010
 6300/27456 [=====>........................] - ETA: 24s - loss: 1.0309 - acc: 0.5006
 6400/27456 [=====>........................] - ETA: 23s - loss: 1.0308 - acc: 0.5005
 6500/27456 [======>.......................] - ETA: 23s - loss: 1.0315 - acc: 0.4995
 6600/27456 [======>.......................] - ETA: 23s - loss: 1.0317 - acc: 0.4989
 6700/27456 [======>.......................] - ETA: 23s - loss: 1.0312 - acc: 0.4997
 6800/27456 [======>.......................] - ETA: 23s - loss: 1.0310 - acc: 0.4994
 6900/27456 [======>.......................] - ETA: 22s - loss: 1.0312 - acc: 0.4994
 7000/27456 [======>.......................] - ETA: 22s - loss: 1.0307 - acc: 0.5004
 7100/27456 [======>.......................] - ETA: 22s - loss: 1.0298 - acc: 0.5013
 7200/27456 [======>.......................] - ETA: 22s - loss: 1.0290 - acc: 0.5014
 7300/27456 [======>.......................] - ETA: 22s - loss: 1.0290 - acc: 0.5012
 7400/27456 [=======>......................] - ETA: 21s - loss: 1.0294 - acc: 0.5007
 7500/27456 [=======>......................] - ETA: 21s - loss: 1.0295 - acc: 0.5007
 7600/27456 [=======>......................] - ETA: 21s - loss: 1.0297 - acc: 0.4999
 7700/27456 [=======>......................] - ETA: 21s - loss: 1.0290 - acc: 0.5004
 7800/27456 [=======>......................] - ETA: 21s - loss: 1.0292 - acc: 0.4997
 7900/27456 [=======>......................] - ETA: 21s - loss: 1.0292 - acc: 0.4994
 8000/27456 [=======>......................] - ETA: 20s - loss: 1.0290 - acc: 0.4999
 8100/27456 [=======>......................] - ETA: 20s - loss: 1.0284 - acc: 0.5002
 8200/27456 [=======>......................] - ETA: 20s - loss: 1.0291 - acc: 0.4989
 8300/27456 [========>.....................] - ETA: 20s - loss: 1.0287 - acc: 0.4990
 8400/27456 [========>.....................] - ETA: 20s - loss: 1.0293 - acc: 0.4982
 8500/27456 [========>.....................] - ETA: 20s - loss: 1.0291 - acc: 0.4986
 8600/27456 [========>.....................] - ETA: 19s - loss: 1.0294 - acc: 0.4979
 8700/27456 [========>.....................] - ETA: 19s - loss: 1.0289 - acc: 0.4987
 8800/27456 [========>.....................] - ETA: 19s - loss: 1.0286 - acc: 0.4992
 8900/27456 [========>.....................] - ETA: 19s - loss: 1.0282 - acc: 0.4997
 9000/27456 [========>.....................] - ETA: 19s - loss: 1.0284 - acc: 0.4994
 9100/27456 [========>.....................] - ETA: 19s - loss: 1.0284 - acc: 0.4996
 9200/27456 [=========>....................] - ETA: 19s - loss: 1.0282 - acc: 0.5000
 9300/27456 [=========>....................] - ETA: 18s - loss: 1.0289 - acc: 0.4989
 9400/27456 [=========>....................] - ETA: 18s - loss: 1.0284 - acc: 0.4995
 9500/27456 [=========>....................] - ETA: 18s - loss: 1.0280 - acc: 0.5003
 9600/27456 [=========>....................] - ETA: 18s - loss: 1.0282 - acc: 0.5002
 9700/27456 [=========>....................] - ETA: 18s - loss: 1.0282 - acc: 0.5000
 9800/27456 [=========>....................] - ETA: 18s - loss: 1.0278 - acc: 0.5009
 9900/27456 [=========>....................] - ETA: 18s - loss: 1.0268 - acc: 0.5022
10000/27456 [=========>....................] - ETA: 17s - loss: 1.0264 - acc: 0.5029
10100/27456 [==========>...................] - ETA: 17s - loss: 1.0260 - acc: 0.5033
10200/27456 [==========>...................] - ETA: 17s - loss: 1.0258 - acc: 0.5034
10300/27456 [==========>...................] - ETA: 17s - loss: 1.0254 - acc: 0.5039
10400/27456 [==========>...................] - ETA: 17s - loss: 1.0257 - acc: 0.5037
10500/27456 [==========>...................] - ETA: 17s - loss: 1.0257 - acc: 0.5039
10600/27456 [==========>...................] - ETA: 17s - loss: 1.0253 - acc: 0.5042
10700/27456 [==========>...................] - ETA: 16s - loss: 1.0259 - acc: 0.5035
10800/27456 [==========>...................] - ETA: 16s - loss: 1.0258 - acc: 0.5035
10900/27456 [==========>...................] - ETA: 16s - loss: 1.0251 - acc: 0.5040
11000/27456 [===========>..................] - ETA: 16s - loss: 1.0249 - acc: 0.5044
11100/27456 [===========>..................] - ETA: 16s - loss: 1.0250 - acc: 0.5039
11200/27456 [===========>..................] - ETA: 16s - loss: 1.0248 - acc: 0.5040
11300/27456 [===========>..................] - ETA: 16s - loss: 1.0259 - acc: 0.5041
11400/27456 [===========>..................] - ETA: 16s - loss: 1.0266 - acc: 0.5032
11500/27456 [===========>..................] - ETA: 15s - loss: 1.0265 - acc: 0.5031
11600/27456 [===========>..................] - ETA: 15s - loss: 1.0261 - acc: 0.5030
11700/27456 [===========>..................] - ETA: 15s - loss: 1.0257 - acc: 0.5036
11800/27456 [===========>..................] - ETA: 15s - loss: 1.0271 - acc: 0.5034
11900/27456 [============>.................] - ETA: 15s - loss: 1.0275 - acc: 0.5028
12000/27456 [============>.................] - ETA: 15s - loss: 1.0275 - acc: 0.5028
12100/27456 [============>.................] - ETA: 15s - loss: 1.0274 - acc: 0.5029
12200/27456 [============>.................] - ETA: 15s - loss: 1.0277 - acc: 0.5024
12300/27456 [============>.................] - ETA: 15s - loss: 1.0275 - acc: 0.5024
12400/27456 [============>.................] - ETA: 14s - loss: 1.0272 - acc: 0.5027
12500/27456 [============>.................] - ETA: 14s - loss: 1.0269 - acc: 0.5026
12600/27456 [============>.................] - ETA: 14s - loss: 1.0273 - acc: 0.5025
12700/27456 [============>.................] - ETA: 14s - loss: 1.0268 - acc: 0.5029
12800/27456 [============>.................] - ETA: 14s - loss: 1.0269 - acc: 0.5029
12900/27456 [=============>................] - ETA: 14s - loss: 1.0269 - acc: 0.5029
13000/27456 [=============>................] - ETA: 14s - loss: 1.0268 - acc: 0.5032
13100/27456 [=============>................] - ETA: 14s - loss: 1.0283 - acc: 0.5025
13200/27456 [=============>................] - ETA: 13s - loss: 1.0282 - acc: 0.5028
13300/27456 [=============>................] - ETA: 13s - loss: 1.0284 - acc: 0.5025
13400/27456 [=============>................] - ETA: 13s - loss: 1.0282 - acc: 0.5028
13500/27456 [=============>................] - ETA: 13s - loss: 1.0283 - acc: 0.5025
13600/27456 [=============>................] - ETA: 13s - loss: 1.0288 - acc: 0.5029
13700/27456 [=============>................] - ETA: 13s - loss: 1.0287 - acc: 0.5028
13800/27456 [==============>...............] - ETA: 13s - loss: 1.0288 - acc: 0.5025
13900/27456 [==============>...............] - ETA: 13s - loss: 1.0290 - acc: 0.5022
14000/27456 [==============>...............] - ETA: 13s - loss: 1.0290 - acc: 0.5021
14100/27456 [==============>...............] - ETA: 12s - loss: 1.0286 - acc: 0.5026
14200/27456 [==============>...............] - ETA: 12s - loss: 1.0283 - acc: 0.5027
14300/27456 [==============>...............] - ETA: 12s - loss: 1.0281 - acc: 0.5029
14400/27456 [==============>...............] - ETA: 12s - loss: 1.0279 - acc: 0.5029
14500/27456 [==============>...............] - ETA: 12s - loss: 1.0281 - acc: 0.5026
14600/27456 [==============>...............] - ETA: 12s - loss: 1.0286 - acc: 0.5020
14700/27456 [===============>..............] - ETA: 12s - loss: 1.0288 - acc: 0.5018
14800/27456 [===============>..............] - ETA: 12s - loss: 1.0286 - acc: 0.5020
14900/27456 [===============>..............] - ETA: 12s - loss: 1.0282 - acc: 0.5025
15000/27456 [===============>..............] - ETA: 11s - loss: 1.0285 - acc: 0.5023
15100/27456 [===============>..............] - ETA: 11s - loss: 1.0289 - acc: 0.5019
15200/27456 [===============>..............] - ETA: 11s - loss: 1.0288 - acc: 0.5019
15300/27456 [===============>..............] - ETA: 11s - loss: 1.0288 - acc: 0.5019
15400/27456 [===============>..............] - ETA: 11s - loss: 1.0285 - acc: 0.5023
15500/27456 [===============>..............] - ETA: 11s - loss: 1.0283 - acc: 0.5023
15600/27456 [================>.............] - ETA: 11s - loss: 1.0295 - acc: 0.5021
15700/27456 [================>.............] - ETA: 11s - loss: 1.0294 - acc: 0.5022
15800/27456 [================>.............] - ETA: 11s - loss: 1.0295 - acc: 0.5021
15900/27456 [================>.............] - ETA: 11s - loss: 1.0292 - acc: 0.5024
16000/27456 [================>.............] - ETA: 10s - loss: 1.0289 - acc: 0.5024
16100/27456 [================>.............] - ETA: 10s - loss: 1.0286 - acc: 0.5027
16200/27456 [================>.............] - ETA: 10s - loss: 1.0288 - acc: 0.5024
16300/27456 [================>.............] - ETA: 10s - loss: 1.0287 - acc: 0.5025
16400/27456 [================>.............] - ETA: 10s - loss: 1.0281 - acc: 0.5030
16500/27456 [=================>............] - ETA: 10s - loss: 1.0281 - acc: 0.5028
16600/27456 [=================>............] - ETA: 10s - loss: 1.0280 - acc: 0.5028
16700/27456 [=================>............] - ETA: 10s - loss: 1.0275 - acc: 0.5035
16800/27456 [=================>............] - ETA: 10s - loss: 1.0272 - acc: 0.5040
16900/27456 [=================>............] - ETA: 10s - loss: 1.0273 - acc: 0.5036
17000/27456 [=================>............] - ETA: 9s - loss: 1.0273 - acc: 0.5038 
17100/27456 [=================>............] - ETA: 9s - loss: 1.0275 - acc: 0.5040
17200/27456 [=================>............] - ETA: 9s - loss: 1.0273 - acc: 0.5043
17300/27456 [=================>............] - ETA: 9s - loss: 1.0272 - acc: 0.5046
17400/27456 [==================>...........] - ETA: 9s - loss: 1.0272 - acc: 0.5045
17500/27456 [==================>...........] - ETA: 9s - loss: 1.0276 - acc: 0.5039
17600/27456 [==================>...........] - ETA: 9s - loss: 1.0271 - acc: 0.5045
17700/27456 [==================>...........] - ETA: 9s - loss: 1.0280 - acc: 0.5045
17800/27456 [==================>...........] - ETA: 9s - loss: 1.0289 - acc: 0.5042
17900/27456 [==================>...........] - ETA: 8s - loss: 1.0285 - acc: 0.5047
18000/27456 [==================>...........] - ETA: 8s - loss: 1.0285 - acc: 0.5047
18100/27456 [==================>...........] - ETA: 8s - loss: 1.0283 - acc: 0.5049
18200/27456 [==================>...........] - ETA: 8s - loss: 1.0282 - acc: 0.5048
18300/27456 [==================>...........] - ETA: 8s - loss: 1.0285 - acc: 0.5045
18400/27456 [===================>..........] - ETA: 8s - loss: 1.0287 - acc: 0.5043
18500/27456 [===================>..........] - ETA: 8s - loss: 1.0286 - acc: 0.5042
18600/27456 [===================>..........] - ETA: 8s - loss: 1.0298 - acc: 0.5039
18700/27456 [===================>..........] - ETA: 8s - loss: 1.0296 - acc: 0.5041
18800/27456 [===================>..........] - ETA: 8s - loss: 1.0296 - acc: 0.5043
18900/27456 [===================>..........] - ETA: 8s - loss: 1.0295 - acc: 0.5044
19000/27456 [===================>..........] - ETA: 7s - loss: 1.0292 - acc: 0.5048
19100/27456 [===================>..........] - ETA: 7s - loss: 1.0301 - acc: 0.5046
19200/27456 [===================>..........] - ETA: 7s - loss: 1.0298 - acc: 0.5048
19300/27456 [====================>.........] - ETA: 7s - loss: 1.0298 - acc: 0.5049
19400/27456 [====================>.........] - ETA: 7s - loss: 1.0298 - acc: 0.5048
19500/27456 [====================>.........] - ETA: 7s - loss: 1.0298 - acc: 0.5046
19600/27456 [====================>.........] - ETA: 7s - loss: 1.0298 - acc: 0.5046
19700/27456 [====================>.........] - ETA: 7s - loss: 1.0299 - acc: 0.5044
19800/27456 [====================>.........] - ETA: 7s - loss: 1.0300 - acc: 0.5043
19900/27456 [====================>.........] - ETA: 7s - loss: 1.0300 - acc: 0.5043
20000/27456 [====================>.........] - ETA: 6s - loss: 1.0299 - acc: 0.5042
20100/27456 [====================>.........] - ETA: 6s - loss: 1.0298 - acc: 0.5042
20200/27456 [=====================>........] - ETA: 6s - loss: 1.0296 - acc: 0.5044
20300/27456 [=====================>........] - ETA: 6s - loss: 1.0297 - acc: 0.5041
20400/27456 [=====================>........] - ETA: 6s - loss: 1.0295 - acc: 0.5044
20500/27456 [=====================>........] - ETA: 6s - loss: 1.0292 - acc: 0.5046
20600/27456 [=====================>........] - ETA: 6s - loss: 1.0288 - acc: 0.5050
20700/27456 [=====================>........] - ETA: 6s - loss: 1.0287 - acc: 0.5051
20800/27456 [=====================>........] - ETA: 6s - loss: 1.0285 - acc: 0.5054
20900/27456 [=====================>........] - ETA: 6s - loss: 1.0286 - acc: 0.5053
21000/27456 [=====================>........] - ETA: 5s - loss: 1.0285 - acc: 0.5053
21100/27456 [======================>.......] - ETA: 5s - loss: 1.0286 - acc: 0.5051
21200/27456 [======================>.......] - ETA: 5s - loss: 1.0285 - acc: 0.5053
21300/27456 [======================>.......] - ETA: 5s - loss: 1.0284 - acc: 0.5055
21400/27456 [======================>.......] - ETA: 5s - loss: 1.0283 - acc: 0.5056
21500/27456 [======================>.......] - ETA: 5s - loss: 1.0286 - acc: 0.5052
21600/27456 [======================>.......] - ETA: 5s - loss: 1.0285 - acc: 0.5053
21700/27456 [======================>.......] - ETA: 5s - loss: 1.0282 - acc: 0.5054
21800/27456 [======================>.......] - ETA: 5s - loss: 1.0278 - acc: 0.5058
21900/27456 [======================>.......] - ETA: 5s - loss: 1.0278 - acc: 0.5058
22000/27456 [=======================>......] - ETA: 5s - loss: 1.0278 - acc: 0.5056
22100/27456 [=======================>......] - ETA: 4s - loss: 1.0277 - acc: 0.5055
22200/27456 [=======================>......] - ETA: 4s - loss: 1.0278 - acc: 0.5054
22300/27456 [=======================>......] - ETA: 4s - loss: 1.0277 - acc: 0.5056
22400/27456 [=======================>......] - ETA: 4s - loss: 1.0275 - acc: 0.5058
22500/27456 [=======================>......] - ETA: 4s - loss: 1.0273 - acc: 0.5058
22600/27456 [=======================>......] - ETA: 4s - loss: 1.0270 - acc: 0.5062
22700/27456 [=======================>......] - ETA: 4s - loss: 1.0265 - acc: 0.5065
22800/27456 [=======================>......] - ETA: 4s - loss: 1.0264 - acc: 0.5066
22900/27456 [========================>.....] - ETA: 4s - loss: 1.0264 - acc: 0.5067
23000/27456 [========================>.....] - ETA: 4s - loss: 1.0264 - acc: 0.5068
23100/27456 [========================>.....] - ETA: 3s - loss: 1.0264 - acc: 0.5066
23200/27456 [========================>.....] - ETA: 3s - loss: 1.0261 - acc: 0.5069
23300/27456 [========================>.....] - ETA: 3s - loss: 1.0262 - acc: 0.5067
23400/27456 [========================>.....] - ETA: 3s - loss: 1.0262 - acc: 0.5067
23500/27456 [========================>.....] - ETA: 3s - loss: 1.0260 - acc: 0.5069
23600/27456 [========================>.....] - ETA: 3s - loss: 1.0258 - acc: 0.5070
23700/27456 [========================>.....] - ETA: 3s - loss: 1.0256 - acc: 0.5071
23800/27456 [=========================>....] - ETA: 3s - loss: 1.0258 - acc: 0.5070
23900/27456 [=========================>....] - ETA: 3s - loss: 1.0259 - acc: 0.5069
24000/27456 [=========================>....] - ETA: 3s - loss: 1.0257 - acc: 0.5070
24100/27456 [=========================>....] - ETA: 3s - loss: 1.0256 - acc: 0.5072
24200/27456 [=========================>....] - ETA: 2s - loss: 1.0256 - acc: 0.5072
24300/27456 [=========================>....] - ETA: 2s - loss: 1.0255 - acc: 0.5074
24400/27456 [=========================>....] - ETA: 2s - loss: 1.0255 - acc: 0.5074
24500/27456 [=========================>....] - ETA: 2s - loss: 1.0256 - acc: 0.5073
24600/27456 [=========================>....] - ETA: 2s - loss: 1.0253 - acc: 0.5076
24700/27456 [=========================>....] - ETA: 2s - loss: 1.0250 - acc: 0.5080
24800/27456 [==========================>...] - ETA: 2s - loss: 1.0250 - acc: 0.5080
24900/27456 [==========================>...] - ETA: 2s - loss: 1.0250 - acc: 0.5079
25000/27456 [==========================>...] - ETA: 2s - loss: 1.0251 - acc: 0.5078
25100/27456 [==========================>...] - ETA: 2s - loss: 1.0247 - acc: 0.5083
25200/27456 [==========================>...] - ETA: 2s - loss: 1.0246 - acc: 0.5083
25300/27456 [==========================>...] - ETA: 1s - loss: 1.0244 - acc: 0.5085
25400/27456 [==========================>...] - ETA: 1s - loss: 1.0244 - acc: 0.5085
25500/27456 [==========================>...] - ETA: 1s - loss: 1.0244 - acc: 0.5083
25600/27456 [==========================>...] - ETA: 1s - loss: 1.0242 - acc: 0.5085
25700/27456 [===========================>..] - ETA: 1s - loss: 1.0241 - acc: 0.5086
25800/27456 [===========================>..] - ETA: 1s - loss: 1.0240 - acc: 0.5087
25900/27456 [===========================>..] - ETA: 1s - loss: 1.0239 - acc: 0.5088
26000/27456 [===========================>..] - ETA: 1s - loss: 1.0239 - acc: 0.5088
26100/27456 [===========================>..] - ETA: 1s - loss: 1.0250 - acc: 0.5087
26200/27456 [===========================>..] - ETA: 1s - loss: 1.0248 - acc: 0.5088
26300/27456 [===========================>..] - ETA: 1s - loss: 1.0249 - acc: 0.5087
26400/27456 [===========================>..] - ETA: 0s - loss: 1.0251 - acc: 0.5084
26500/27456 [===========================>..] - ETA: 0s - loss: 1.0249 - acc: 0.5087
26600/27456 [============================>.] - ETA: 0s - loss: 1.0248 - acc: 0.5089
26700/27456 [============================>.] - ETA: 0s - loss: 1.0248 - acc: 0.5089
26800/27456 [============================>.] - ETA: 0s - loss: 1.0247 - acc: 0.5091
26900/27456 [============================>.] - ETA: 0s - loss: 1.0246 - acc: 0.5091
27000/27456 [============================>.] - ETA: 0s - loss: 1.0247 - acc: 0.5090
27100/27456 [============================>.] - ETA: 0s - loss: 1.0248 - acc: 0.5088
27200/27456 [============================>.] - ETA: 0s - loss: 1.0246 - acc: 0.5089
27300/27456 [============================>.] - ETA: 0s - loss: 1.0247 - acc: 0.5088
27400/27456 [============================>.] - ETA: 0s - loss: 1.0246 - acc: 0.5091
27456/27456 [==============================] - 25s 904us/step - loss: 1.0246 - acc: 0.5090
Epoch 2/30

  100/27456 [..............................] - ETA: 22s - loss: 1.0152 - acc: 0.5200
  200/27456 [..............................] - ETA: 22s - loss: 0.9906 - acc: 0.5550
  300/27456 [..............................] - ETA: 22s - loss: 1.0061 - acc: 0.5333
  400/27456 [..............................] - ETA: 22s - loss: 1.0031 - acc: 0.5300
  500/27456 [..............................] - ETA: 22s - loss: 1.0068 - acc: 0.5200
  600/27456 [..............................] - ETA: 22s - loss: 1.0072 - acc: 0.5250
  700/27456 [..............................] - ETA: 22s - loss: 1.0083 - acc: 0.5257
  800/27456 [..............................] - ETA: 22s - loss: 1.0139 - acc: 0.5187
  900/27456 [..............................] - ETA: 22s - loss: 1.0200 - acc: 0.5111
 1000/27456 [>.............................] - ETA: 22s - loss: 1.0238 - acc: 0.5070
 1100/27456 [>.............................] - ETA: 21s - loss: 1.0201 - acc: 0.5109
 1200/27456 [>.............................] - ETA: 21s - loss: 1.0175 - acc: 0.5133
 1300/27456 [>.............................] - ETA: 21s - loss: 1.0153 - acc: 0.5146
 1400/27456 [>.............................] - ETA: 21s - loss: 1.0217 - acc: 0.5093
 1500/27456 [>.............................] - ETA: 21s - loss: 1.0216 - acc: 0.5093
 1600/27456 [>.............................] - ETA: 21s - loss: 1.0216 - acc: 0.5081
 1700/27456 [>.............................] - ETA: 21s - loss: 1.0252 - acc: 0.5047
 1800/27456 [>.............................] - ETA: 21s - loss: 1.0264 - acc: 0.5050
 1900/27456 [=>............................] - ETA: 21s - loss: 1.0225 - acc: 0.5084
 2000/27456 [=>............................] - ETA: 21s - loss: 1.0260 - acc: 0.5055

*** WARNING: skipped 648467 bytes of output ***

25800/27456 [===========================>..] - ETA: 1s - loss: 1.1542 - acc: 0.5093
25900/27456 [===========================>..] - ETA: 1s - loss: 1.1536 - acc: 0.5093
26000/27456 [===========================>..] - ETA: 1s - loss: 1.1529 - acc: 0.5096
26100/27456 [===========================>..] - ETA: 1s - loss: 1.1536 - acc: 0.5093
26200/27456 [===========================>..] - ETA: 1s - loss: 1.1537 - acc: 0.5094
26300/27456 [===========================>..] - ETA: 0s - loss: 1.1543 - acc: 0.5093
26400/27456 [===========================>..] - ETA: 0s - loss: 1.1539 - acc: 0.5091
26500/27456 [===========================>..] - ETA: 0s - loss: 1.1543 - acc: 0.5092
26600/27456 [============================>.] - ETA: 0s - loss: 1.1543 - acc: 0.5094
26700/27456 [============================>.] - ETA: 0s - loss: 1.1536 - acc: 0.5094
26800/27456 [============================>.] - ETA: 0s - loss: 1.1535 - acc: 0.5096
26900/27456 [============================>.] - ETA: 0s - loss: 1.1547 - acc: 0.5096
27000/27456 [============================>.] - ETA: 0s - loss: 1.1552 - acc: 0.5095
27100/27456 [============================>.] - ETA: 0s - loss: 1.1548 - acc: 0.5094
27200/27456 [============================>.] - ETA: 0s - loss: 1.1549 - acc: 0.5093
27300/27456 [============================>.] - ETA: 0s - loss: 1.1564 - acc: 0.5089
27400/27456 [============================>.] - ETA: 0s - loss: 1.1556 - acc: 0.5093
27456/27456 [==============================] - 23s 832us/step - loss: 1.1555 - acc: 0.5091
Epoch 30/30

  100/27456 [..............................] - ETA: 22s - loss: 1.2810 - acc: 0.5500
  200/27456 [..............................] - ETA: 22s - loss: 1.2382 - acc: 0.5100
  300/27456 [..............................] - ETA: 22s - loss: 1.2022 - acc: 0.5167
  400/27456 [..............................] - ETA: 22s - loss: 1.1582 - acc: 0.5075
  500/27456 [..............................] - ETA: 22s - loss: 1.1191 - acc: 0.5200
  600/27456 [..............................] - ETA: 22s - loss: 1.1049 - acc: 0.5150
  700/27456 [..............................] - ETA: 22s - loss: 1.1422 - acc: 0.5000
  800/27456 [..............................] - ETA: 22s - loss: 1.1962 - acc: 0.5025
  900/27456 [..............................] - ETA: 21s - loss: 1.2098 - acc: 0.5022
 1000/27456 [>.............................] - ETA: 21s - loss: 1.1989 - acc: 0.5080
 1100/27456 [>.............................] - ETA: 21s - loss: 1.1837 - acc: 0.5000
 1200/27456 [>.............................] - ETA: 21s - loss: 1.1720 - acc: 0.4967
 1300/27456 [>.............................] - ETA: 21s - loss: 1.1659 - acc: 0.5054
 1400/27456 [>.............................] - ETA: 21s - loss: 1.1961 - acc: 0.5071
 1500/27456 [>.............................] - ETA: 21s - loss: 1.1915 - acc: 0.5080
 1600/27456 [>.............................] - ETA: 21s - loss: 1.1776 - acc: 0.5094
 1700/27456 [>.............................] - ETA: 21s - loss: 1.1968 - acc: 0.5047
 1800/27456 [>.............................] - ETA: 21s - loss: 1.1839 - acc: 0.5100
 1900/27456 [=>............................] - ETA: 21s - loss: 1.1830 - acc: 0.5084
 2000/27456 [=>............................] - ETA: 21s - loss: 1.1864 - acc: 0.5020
 2100/27456 [=>............................] - ETA: 20s - loss: 1.1850 - acc: 0.5024
 2200/27456 [=>............................] - ETA: 20s - loss: 1.1901 - acc: 0.5005
 2300/27456 [=>............................] - ETA: 20s - loss: 1.1875 - acc: 0.5030
 2400/27456 [=>............................] - ETA: 20s - loss: 1.1780 - acc: 0.5062
 2500/27456 [=>............................] - ETA: 20s - loss: 1.1708 - acc: 0.5084
 2600/27456 [=>............................] - ETA: 20s - loss: 1.1689 - acc: 0.5104
 2700/27456 [=>............................] - ETA: 20s - loss: 1.1675 - acc: 0.5115
 2800/27456 [==>...........................] - ETA: 20s - loss: 1.1659 - acc: 0.5136
 2900/27456 [==>...........................] - ETA: 20s - loss: 1.1597 - acc: 0.5134
 3000/27456 [==>...........................] - ETA: 20s - loss: 1.1587 - acc: 0.5140
 3100/27456 [==>...........................] - ETA: 20s - loss: 1.1514 - acc: 0.5168
 3200/27456 [==>...........................] - ETA: 20s - loss: 1.1529 - acc: 0.5144
 3300/27456 [==>...........................] - ETA: 19s - loss: 1.1523 - acc: 0.5155
 3400/27456 [==>...........................] - ETA: 19s - loss: 1.1518 - acc: 0.5159
 3500/27456 [==>...........................] - ETA: 19s - loss: 1.1467 - acc: 0.5157
 3600/27456 [==>...........................] - ETA: 19s - loss: 1.1413 - acc: 0.5194
 3700/27456 [===>..........................] - ETA: 19s - loss: 1.1415 - acc: 0.5186
 3800/27456 [===>..........................] - ETA: 19s - loss: 1.1440 - acc: 0.5168
 3900/27456 [===>..........................] - ETA: 19s - loss: 1.1423 - acc: 0.5195
 4000/27456 [===>..........................] - ETA: 19s - loss: 1.1463 - acc: 0.5180
 4100/27456 [===>..........................] - ETA: 19s - loss: 1.1498 - acc: 0.5178
 4200/27456 [===>..........................] - ETA: 19s - loss: 1.1453 - acc: 0.5190
 4300/27456 [===>..........................] - ETA: 19s - loss: 1.1536 - acc: 0.5181
 4400/27456 [===>..........................] - ETA: 19s - loss: 1.1511 - acc: 0.5164
 4500/27456 [===>..........................] - ETA: 19s - loss: 1.1591 - acc: 0.5153
 4600/27456 [====>.........................] - ETA: 18s - loss: 1.1596 - acc: 0.5150
 4700/27456 [====>.........................] - ETA: 18s - loss: 1.1556 - acc: 0.5155
 4800/27456 [====>.........................] - ETA: 18s - loss: 1.1646 - acc: 0.5160
 4900/27456 [====>.........................] - ETA: 18s - loss: 1.1614 - acc: 0.5157
 5000/27456 [====>.........................] - ETA: 18s - loss: 1.1593 - acc: 0.5140
 5100/27456 [====>.........................] - ETA: 18s - loss: 1.1628 - acc: 0.5131
 5200/27456 [====>.........................] - ETA: 18s - loss: 1.1589 - acc: 0.5137
 5300/27456 [====>.........................] - ETA: 18s - loss: 1.1556 - acc: 0.5142
 5400/27456 [====>.........................] - ETA: 18s - loss: 1.1552 - acc: 0.5150
 5500/27456 [=====>........................] - ETA: 18s - loss: 1.1556 - acc: 0.5147
 5600/27456 [=====>........................] - ETA: 18s - loss: 1.1609 - acc: 0.5141
 5700/27456 [=====>........................] - ETA: 18s - loss: 1.1660 - acc: 0.5140
 5800/27456 [=====>........................] - ETA: 17s - loss: 1.1635 - acc: 0.5138
 5900/27456 [=====>........................] - ETA: 17s - loss: 1.1624 - acc: 0.5142
 6000/27456 [=====>........................] - ETA: 17s - loss: 1.1643 - acc: 0.5152
 6100/27456 [=====>........................] - ETA: 17s - loss: 1.1627 - acc: 0.5164
 6200/27456 [=====>........................] - ETA: 17s - loss: 1.1625 - acc: 0.5163
 6300/27456 [=====>........................] - ETA: 17s - loss: 1.1626 - acc: 0.5163
 6400/27456 [=====>........................] - ETA: 17s - loss: 1.1603 - acc: 0.5162
 6500/27456 [======>.......................] - ETA: 17s - loss: 1.1648 - acc: 0.5165
 6600/27456 [======>.......................] - ETA: 17s - loss: 1.1626 - acc: 0.5159
 6700/27456 [======>.......................] - ETA: 17s - loss: 1.1627 - acc: 0.5161
 6800/27456 [======>.......................] - ETA: 17s - loss: 1.1667 - acc: 0.5166
 6900/27456 [======>.......................] - ETA: 17s - loss: 1.1739 - acc: 0.5157
 7000/27456 [======>.......................] - ETA: 16s - loss: 1.1786 - acc: 0.5150
 7100/27456 [======>.......................] - ETA: 16s - loss: 1.1781 - acc: 0.5154
 7200/27456 [======>.......................] - ETA: 16s - loss: 1.1801 - acc: 0.5149
 7300/27456 [======>.......................] - ETA: 16s - loss: 1.1780 - acc: 0.5148
 7400/27456 [=======>......................] - ETA: 16s - loss: 1.1746 - acc: 0.5162
 7500/27456 [=======>......................] - ETA: 16s - loss: 1.1760 - acc: 0.5163
 7600/27456 [=======>......................] - ETA: 16s - loss: 1.1758 - acc: 0.5159
 7700/27456 [=======>......................] - ETA: 16s - loss: 1.1738 - acc: 0.5157
 7800/27456 [=======>......................] - ETA: 16s - loss: 1.1742 - acc: 0.5146
 7900/27456 [=======>......................] - ETA: 16s - loss: 1.1742 - acc: 0.5141
 8000/27456 [=======>......................] - ETA: 16s - loss: 1.1737 - acc: 0.5146
 8100/27456 [=======>......................] - ETA: 16s - loss: 1.1788 - acc: 0.5148
 8200/27456 [=======>......................] - ETA: 15s - loss: 1.1778 - acc: 0.5152
 8300/27456 [========>.....................] - ETA: 15s - loss: 1.1818 - acc: 0.5143
 8400/27456 [========>.....................] - ETA: 15s - loss: 1.1834 - acc: 0.5142
 8500/27456 [========>.....................] - ETA: 15s - loss: 1.1815 - acc: 0.5145
 8600/27456 [========>.....................] - ETA: 15s - loss: 1.1789 - acc: 0.5150
 8700/27456 [========>.....................] - ETA: 15s - loss: 1.1766 - acc: 0.5153
 8800/27456 [========>.....................] - ETA: 15s - loss: 1.1755 - acc: 0.5143
 8900/27456 [========>.....................] - ETA: 15s - loss: 1.1735 - acc: 0.5140
 9000/27456 [========>.....................] - ETA: 15s - loss: 1.1734 - acc: 0.5136
 9100/27456 [========>.....................] - ETA: 15s - loss: 1.1709 - acc: 0.5142
 9200/27456 [=========>....................] - ETA: 15s - loss: 1.1703 - acc: 0.5133
 9300/27456 [=========>....................] - ETA: 15s - loss: 1.1696 - acc: 0.5123
 9400/27456 [=========>....................] - ETA: 14s - loss: 1.1679 - acc: 0.5123
 9500/27456 [=========>....................] - ETA: 14s - loss: 1.1683 - acc: 0.5120
 9600/27456 [=========>....................] - ETA: 14s - loss: 1.1690 - acc: 0.5107
 9700/27456 [=========>....................] - ETA: 14s - loss: 1.1669 - acc: 0.5108
 9800/27456 [=========>....................] - ETA: 14s - loss: 1.1665 - acc: 0.5110
 9900/27456 [=========>....................] - ETA: 14s - loss: 1.1695 - acc: 0.5108
10000/27456 [=========>....................] - ETA: 14s - loss: 1.1681 - acc: 0.5108
10100/27456 [==========>...................] - ETA: 14s - loss: 1.1680 - acc: 0.5110
10200/27456 [==========>...................] - ETA: 14s - loss: 1.1673 - acc: 0.5101
10300/27456 [==========>...................] - ETA: 14s - loss: 1.1673 - acc: 0.5098
10400/27456 [==========>...................] - ETA: 14s - loss: 1.1684 - acc: 0.5101
10500/27456 [==========>...................] - ETA: 14s - loss: 1.1669 - acc: 0.5101
10600/27456 [==========>...................] - ETA: 13s - loss: 1.1646 - acc: 0.5109
10700/27456 [==========>...................] - ETA: 13s - loss: 1.1687 - acc: 0.5107
10800/27456 [==========>...................] - ETA: 13s - loss: 1.1686 - acc: 0.5105
10900/27456 [==========>...................] - ETA: 13s - loss: 1.1693 - acc: 0.5109
11000/27456 [===========>..................] - ETA: 13s - loss: 1.1679 - acc: 0.5109
11100/27456 [===========>..................] - ETA: 13s - loss: 1.1666 - acc: 0.5105
11200/27456 [===========>..................] - ETA: 13s - loss: 1.1691 - acc: 0.5104
11300/27456 [===========>..................] - ETA: 13s - loss: 1.1702 - acc: 0.5101
11400/27456 [===========>..................] - ETA: 13s - loss: 1.1710 - acc: 0.5108
11500/27456 [===========>..................] - ETA: 13s - loss: 1.1712 - acc: 0.5104
11600/27456 [===========>..................] - ETA: 13s - loss: 1.1741 - acc: 0.5099
11700/27456 [===========>..................] - ETA: 13s - loss: 1.1739 - acc: 0.5097
11800/27456 [===========>..................] - ETA: 12s - loss: 1.1735 - acc: 0.5099
11900/27456 [============>.................] - ETA: 12s - loss: 1.1746 - acc: 0.5096
12000/27456 [============>.................] - ETA: 12s - loss: 1.1731 - acc: 0.5091
12100/27456 [============>.................] - ETA: 12s - loss: 1.1731 - acc: 0.5092
12200/27456 [============>.................] - ETA: 12s - loss: 1.1741 - acc: 0.5093
12300/27456 [============>.................] - ETA: 12s - loss: 1.1738 - acc: 0.5098
12400/27456 [============>.................] - ETA: 12s - loss: 1.1739 - acc: 0.5098
12500/27456 [============>.................] - ETA: 12s - loss: 1.1766 - acc: 0.5093
12600/27456 [============>.................] - ETA: 12s - loss: 1.1787 - acc: 0.5095
12700/27456 [============>.................] - ETA: 12s - loss: 1.1804 - acc: 0.5099
12800/27456 [============>.................] - ETA: 12s - loss: 1.1788 - acc: 0.5101
12900/27456 [=============>................] - ETA: 12s - loss: 1.1799 - acc: 0.5095
13000/27456 [=============>................] - ETA: 11s - loss: 1.1799 - acc: 0.5096
13100/27456 [=============>................] - ETA: 11s - loss: 1.1831 - acc: 0.5096
13200/27456 [=============>................] - ETA: 11s - loss: 1.1818 - acc: 0.5099
13300/27456 [=============>................] - ETA: 11s - loss: 1.1815 - acc: 0.5102
13400/27456 [=============>................] - ETA: 11s - loss: 1.1811 - acc: 0.5104
13500/27456 [=============>................] - ETA: 11s - loss: 1.1800 - acc: 0.5099
13600/27456 [=============>................] - ETA: 11s - loss: 1.1821 - acc: 0.5101
13700/27456 [=============>................] - ETA: 11s - loss: 1.1821 - acc: 0.5099
13800/27456 [==============>...............] - ETA: 11s - loss: 1.1829 - acc: 0.5102
13900/27456 [==============>...............] - ETA: 11s - loss: 1.1818 - acc: 0.5103
14000/27456 [==============>...............] - ETA: 11s - loss: 1.1827 - acc: 0.5099
14100/27456 [==============>...............] - ETA: 11s - loss: 1.1822 - acc: 0.5104
14200/27456 [==============>...............] - ETA: 11s - loss: 1.1832 - acc: 0.5101
14300/27456 [==============>...............] - ETA: 10s - loss: 1.1822 - acc: 0.5100
14400/27456 [==============>...............] - ETA: 10s - loss: 1.1838 - acc: 0.5105
14500/27456 [==============>...............] - ETA: 10s - loss: 1.1828 - acc: 0.5105
14600/27456 [==============>...............] - ETA: 10s - loss: 1.1828 - acc: 0.5103
14700/27456 [===============>..............] - ETA: 10s - loss: 1.1813 - acc: 0.5107
14800/27456 [===============>..............] - ETA: 10s - loss: 1.1811 - acc: 0.5107
14900/27456 [===============>..............] - ETA: 10s - loss: 1.1800 - acc: 0.5105
15000/27456 [===============>..............] - ETA: 10s - loss: 1.1801 - acc: 0.5103
15100/27456 [===============>..............] - ETA: 10s - loss: 1.1824 - acc: 0.5098
15200/27456 [===============>..............] - ETA: 10s - loss: 1.1824 - acc: 0.5097
15300/27456 [===============>..............] - ETA: 10s - loss: 1.1815 - acc: 0.5096
15400/27456 [===============>..............] - ETA: 10s - loss: 1.1801 - acc: 0.5101
15500/27456 [===============>..............] - ETA: 9s - loss: 1.1805 - acc: 0.5095 
15600/27456 [================>.............] - ETA: 9s - loss: 1.1815 - acc: 0.5094
15700/27456 [================>.............] - ETA: 9s - loss: 1.1816 - acc: 0.5092
15800/27456 [================>.............] - ETA: 9s - loss: 1.1815 - acc: 0.5091
15900/27456 [================>.............] - ETA: 9s - loss: 1.1822 - acc: 0.5092
16000/27456 [================>.............] - ETA: 9s - loss: 1.1839 - acc: 0.5094
16100/27456 [================>.............] - ETA: 9s - loss: 1.1849 - acc: 0.5091
16200/27456 [================>.............] - ETA: 9s - loss: 1.1847 - acc: 0.5090
16300/27456 [================>.............] - ETA: 9s - loss: 1.1856 - acc: 0.5086
16400/27456 [================>.............] - ETA: 9s - loss: 1.1857 - acc: 0.5082
16500/27456 [=================>............] - ETA: 9s - loss: 1.1859 - acc: 0.5079
16600/27456 [=================>............] - ETA: 9s - loss: 1.1874 - acc: 0.5081
16700/27456 [=================>............] - ETA: 8s - loss: 1.1882 - acc: 0.5081
16800/27456 [=================>............] - ETA: 8s - loss: 1.1880 - acc: 0.5079
16900/27456 [=================>............] - ETA: 8s - loss: 1.1888 - acc: 0.5077
17000/27456 [=================>............] - ETA: 8s - loss: 1.1887 - acc: 0.5077
17100/27456 [=================>............] - ETA: 8s - loss: 1.1884 - acc: 0.5081
17200/27456 [=================>............] - ETA: 8s - loss: 1.1882 - acc: 0.5083
17300/27456 [=================>............] - ETA: 8s - loss: 1.1890 - acc: 0.5083
17400/27456 [==================>...........] - ETA: 8s - loss: 1.1893 - acc: 0.5088
17500/27456 [==================>...........] - ETA: 8s - loss: 1.1883 - acc: 0.5087
17600/27456 [==================>...........] - ETA: 8s - loss: 1.1878 - acc: 0.5080
17700/27456 [==================>...........] - ETA: 8s - loss: 1.1866 - acc: 0.5082
17800/27456 [==================>...........] - ETA: 8s - loss: 1.1882 - acc: 0.5081
17900/27456 [==================>...........] - ETA: 7s - loss: 1.1889 - acc: 0.5081
18000/27456 [==================>...........] - ETA: 7s - loss: 1.1888 - acc: 0.5081
18100/27456 [==================>...........] - ETA: 7s - loss: 1.1886 - acc: 0.5081
18200/27456 [==================>...........] - ETA: 7s - loss: 1.1886 - acc: 0.5079
18300/27456 [==================>...........] - ETA: 7s - loss: 1.1885 - acc: 0.5079
18400/27456 [===================>..........] - ETA: 7s - loss: 1.1882 - acc: 0.5080
18500/27456 [===================>..........] - ETA: 7s - loss: 1.1889 - acc: 0.5079
18600/27456 [===================>..........] - ETA: 7s - loss: 1.1883 - acc: 0.5076
18700/27456 [===================>..........] - ETA: 7s - loss: 1.1897 - acc: 0.5077
18800/27456 [===================>..........] - ETA: 7s - loss: 1.1887 - acc: 0.5076
18900/27456 [===================>..........] - ETA: 7s - loss: 1.1884 - acc: 0.5078
19000/27456 [===================>..........] - ETA: 7s - loss: 1.1874 - acc: 0.5078
19100/27456 [===================>..........] - ETA: 6s - loss: 1.1869 - acc: 0.5073
19200/27456 [===================>..........] - ETA: 6s - loss: 1.1867 - acc: 0.5072
19300/27456 [====================>.........] - ETA: 6s - loss: 1.1878 - acc: 0.5068
19400/27456 [====================>.........] - ETA: 6s - loss: 1.1874 - acc: 0.5073
19500/27456 [====================>.........] - ETA: 6s - loss: 1.1864 - acc: 0.5074
19600/27456 [====================>.........] - ETA: 6s - loss: 1.1867 - acc: 0.5069
19700/27456 [====================>.........] - ETA: 6s - loss: 1.1870 - acc: 0.5073
19800/27456 [====================>.........] - ETA: 6s - loss: 1.1871 - acc: 0.5074
19900/27456 [====================>.........] - ETA: 6s - loss: 1.1870 - acc: 0.5075
20000/27456 [====================>.........] - ETA: 6s - loss: 1.1861 - acc: 0.5074
20100/27456 [====================>.........] - ETA: 6s - loss: 1.1859 - acc: 0.5074
20200/27456 [=====================>........] - ETA: 6s - loss: 1.1872 - acc: 0.5075
20300/27456 [=====================>........] - ETA: 5s - loss: 1.1868 - acc: 0.5078
20400/27456 [=====================>........] - ETA: 5s - loss: 1.1856 - acc: 0.5083
20500/27456 [=====================>........] - ETA: 5s - loss: 1.1861 - acc: 0.5083
20600/27456 [=====================>........] - ETA: 5s - loss: 1.1876 - acc: 0.5080
20700/27456 [=====================>........] - ETA: 5s - loss: 1.1879 - acc: 0.5082
20800/27456 [=====================>........] - ETA: 5s - loss: 1.1877 - acc: 0.5083
20900/27456 [=====================>........] - ETA: 5s - loss: 1.1872 - acc: 0.5088
21000/27456 [=====================>........] - ETA: 5s - loss: 1.1863 - acc: 0.5089
21100/27456 [======================>.......] - ETA: 5s - loss: 1.1858 - acc: 0.5093
21200/27456 [======================>.......] - ETA: 5s - loss: 1.1870 - acc: 0.5095
21300/27456 [======================>.......] - ETA: 5s - loss: 1.1861 - acc: 0.5095
21400/27456 [======================>.......] - ETA: 5s - loss: 1.1862 - acc: 0.5094
21500/27456 [======================>.......] - ETA: 4s - loss: 1.1853 - acc: 0.5094
21600/27456 [======================>.......] - ETA: 4s - loss: 1.1859 - acc: 0.5094
21700/27456 [======================>.......] - ETA: 4s - loss: 1.1859 - acc: 0.5093
21800/27456 [======================>.......] - ETA: 4s - loss: 1.1856 - acc: 0.5094
21900/27456 [======================>.......] - ETA: 4s - loss: 1.1850 - acc: 0.5093
22000/27456 [=======================>......] - ETA: 4s - loss: 1.1883 - acc: 0.5092
22100/27456 [=======================>......] - ETA: 4s - loss: 1.1885 - acc: 0.5088
22200/27456 [=======================>......] - ETA: 4s - loss: 1.1897 - acc: 0.5088
22300/27456 [=======================>......] - ETA: 4s - loss: 1.1900 - acc: 0.5083
22400/27456 [=======================>......] - ETA: 4s - loss: 1.1913 - acc: 0.5081
22500/27456 [=======================>......] - ETA: 4s - loss: 1.1921 - acc: 0.5086
22600/27456 [=======================>......] - ETA: 4s - loss: 1.1926 - acc: 0.5086
22700/27456 [=======================>......] - ETA: 3s - loss: 1.1923 - acc: 0.5087
22800/27456 [=======================>......] - ETA: 3s - loss: 1.1926 - acc: 0.5087
22900/27456 [========================>.....] - ETA: 3s - loss: 1.1941 - acc: 0.5084
23000/27456 [========================>.....] - ETA: 3s - loss: 1.1932 - acc: 0.5085
23100/27456 [========================>.....] - ETA: 3s - loss: 1.1924 - acc: 0.5086
23200/27456 [========================>.....] - ETA: 3s - loss: 1.1922 - acc: 0.5086
23300/27456 [========================>.....] - ETA: 3s - loss: 1.1926 - acc: 0.5088
23400/27456 [========================>.....] - ETA: 3s - loss: 1.1936 - acc: 0.5089
23500/27456 [========================>.....] - ETA: 3s - loss: 1.1943 - acc: 0.5085
23600/27456 [========================>.....] - ETA: 3s - loss: 1.1944 - acc: 0.5081
23700/27456 [========================>.....] - ETA: 3s - loss: 1.1936 - acc: 0.5082
23800/27456 [=========================>....] - ETA: 3s - loss: 1.1935 - acc: 0.5082
23900/27456 [=========================>....] - ETA: 2s - loss: 1.1933 - acc: 0.5083
24000/27456 [=========================>....] - ETA: 2s - loss: 1.1929 - acc: 0.5085
24100/27456 [=========================>....] - ETA: 2s - loss: 1.1940 - acc: 0.5084
24200/27456 [=========================>....] - ETA: 2s - loss: 1.1932 - acc: 0.5083
24300/27456 [=========================>....] - ETA: 2s - loss: 1.1936 - acc: 0.5085
24400/27456 [=========================>....] - ETA: 2s - loss: 1.1938 - acc: 0.5089
24500/27456 [=========================>....] - ETA: 2s - loss: 1.1934 - acc: 0.5085
24600/27456 [=========================>....] - ETA: 2s - loss: 1.1931 - acc: 0.5084
24700/27456 [=========================>....] - ETA: 2s - loss: 1.1931 - acc: 0.5083
24800/27456 [==========================>...] - ETA: 2s - loss: 1.1928 - acc: 0.5085
24900/27456 [==========================>...] - ETA: 2s - loss: 1.1927 - acc: 0.5085
25000/27456 [==========================>...] - ETA: 2s - loss: 1.1918 - acc: 0.5088
25100/27456 [==========================>...] - ETA: 1s - loss: 1.1911 - acc: 0.5088
25200/27456 [==========================>...] - ETA: 1s - loss: 1.1903 - acc: 0.5088
25300/27456 [==========================>...] - ETA: 1s - loss: 1.1895 - acc: 0.5088
25400/27456 [==========================>...] - ETA: 1s - loss: 1.1887 - acc: 0.5089
25500/27456 [==========================>...] - ETA: 1s - loss: 1.1897 - acc: 0.5091
25600/27456 [==========================>...] - ETA: 1s - loss: 1.1889 - acc: 0.5091
25700/27456 [===========================>..] - ETA: 1s - loss: 1.1882 - acc: 0.5091
25800/27456 [===========================>..] - ETA: 1s - loss: 1.1893 - acc: 0.5091
25900/27456 [===========================>..] - ETA: 1s - loss: 1.1897 - acc: 0.5092
26000/27456 [===========================>..] - ETA: 1s - loss: 1.1892 - acc: 0.5088
26100/27456 [===========================>..] - ETA: 1s - loss: 1.1885 - acc: 0.5090
26200/27456 [===========================>..] - ETA: 1s - loss: 1.1890 - acc: 0.5086
26300/27456 [===========================>..] - ETA: 0s - loss: 1.1888 - acc: 0.5086
26400/27456 [===========================>..] - ETA: 0s - loss: 1.1881 - acc: 0.5087
26500/27456 [===========================>..] - ETA: 0s - loss: 1.1879 - acc: 0.5089
26600/27456 [============================>.] - ETA: 0s - loss: 1.1896 - acc: 0.5087
26700/27456 [============================>.] - ETA: 0s - loss: 1.1894 - acc: 0.5088
26800/27456 [============================>.] - ETA: 0s - loss: 1.1893 - acc: 0.5088
26900/27456 [============================>.] - ETA: 0s - loss: 1.1886 - acc: 0.5091
27000/27456 [============================>.] - ETA: 0s - loss: 1.1880 - acc: 0.5089
27100/27456 [============================>.] - ETA: 0s - loss: 1.1884 - acc: 0.5089
27200/27456 [============================>.] - ETA: 0s - loss: 1.1895 - acc: 0.5089
27300/27456 [============================>.] - ETA: 0s - loss: 1.1889 - acc: 0.5087
27400/27456 [============================>.] - ETA: 0s - loss: 1.1887 - acc: 0.5088
27456/27456 [==============================] - 23s 831us/step - loss: 1.1882 - acc: 0.5090
Out[19]: 
('/dbfs/pepar/Cervix Data/params/paramodel14_v12.h5',
 0.004278005856800815,
 40,
 0.16474441403808845,
 0.39799962590853877,
 0.9991091761645295,
 0.5412844030447743)
image_read("/dbfs//Cervix Data/same_aspect/Type_2/115.jpg",0,0).shape
Cancelled

display(spark_image_data)
Cancelled
image_data = spark_image_data.toPandas()
Cancelled
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import pandas as pd
train, valid = train_test_split(image_data, test_size=0.1)

train_reg = train.copy()
train_reg["augmentation"] = 0

train_rot = train.copy()
train_rot["augmentation"] = 1

train_warp = train.copy()
train_warp["augmentation"] = 2

train_aug = pd.concat([train_reg,train_rot,train_warp])

train_flip = train_aug.copy()
train_flip["flip"] = 1

train_nonflip = train_aug.copy()
train_nonflip["flip"] = 0

train_final = shuffle(pd.concat([train_nonflip,train_flip]),random_state =100)
valid["flip"] = 0
valid["augmentation"] = 0
Cancelled
spark_train_data = spark.createDataFrame(train_final)
Cancelled
def resultDLModelHyper(row):
  import pandas as pd
  #spark_image_data = spark.read.parquet("dbfs://Cervix Data/same_aspect/same_aspect/processed_image_metadata.parquet")
  image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")
  from sklearn.model_selection import train_test_split
  from sklearn.utils import shuffle
  import numpy as np
  from keras.layers.advanced_activations import ELU
  import os
  from keras.models import Sequential
  from keras.layers.core import Dense, Activation, Flatten, Dropout
  from keras.layers.convolutional import Convolution2D
  from keras.layers.pooling import MaxPooling2D
  #from keras.layers.advanced_activations import ELU
  #import pandas as pd
  import datetime
  #import matplotlib.image as mpimg
  import numpy as np
  import time
  import datetime
  import json
  import os
  import tensorflow as tf
  #import horovod.tensorflow
  from tensorflow.contrib import learn
  from keras.datasets import mnist
  #import horovod.tensorflow.keras as hvd
  from tensorflow import keras
  from tensorflow.keras import backend as K
  from tensorflow.keras import layers
  from tensorflow.keras.datasets import mnist
  #from sparkdl import HorovodRunner
  import keras
  #from sklearn.utils import shuffle
  import numpy as np
  import cv2
  from tensorflow.python.client import device_lib
  device_lib.list_local_devices()
  from keras.utils import multi_gpu_model
  from keras import backend as K
  K.tensorflow_backend._get_available_gpus()
  
  from keras.models import load_model
  model = load_model(str(row.ModelLocation)[:-7]+"_v11.h5")
  score_train = model.evaluate_generator(train_generator, max_queue_size=10,workers=10, use_multiprocessing=True, verbose=0)
  train_acc = float(score_train[1])
  train_loss = float(score_train[0])
  score = model.evaluate_generator(val_generator, max_queue_size=10,workers=10, use_multiprocessing=True, verbose=0)
  acc = float(score[1])
  loss = float(score[0])
  #acc = 1.0
  #loss = 1.0
  return(str(row.ModelLocation)[:-7]+"_v11.h5", row.Alpha, row.BatchSize, row.DropoutA, row.DropoutB, loss, acc,train_loss,train_acc)
Cancelled
display(models_df_tune)
Cancelled
schema = StructType([
  StructField("ModelLocation", StringType(), False),
  StructField("Alpha", DoubleType(), False),
  StructField("BatchSize", IntegerType(), False),
  StructField("DropoutA", DoubleType(), False),
  StructField("DropoutB", DoubleType(), False),
  StructField("Loss", DoubleType(), False),
  StructField("Accuracy", DoubleType(), False),
  StructField("TrainLoss", DoubleType(), False),
  StructField("TrainAccuracy", DoubleType(), False)
                    ])
Cancelled
models_df_tune = models_df_tune.filter(models_df_tune.ModelLocation != "/dbfs//Cervix Data/params/paramodel7_v10.h5")
pd_results_df = models_df_tune.rdd.map(resultDLModelHyper).toDF(schema).toPandas()
Cancelled
resultDLModelHyper(row.iloc[0])
Cancelled
display(pd_results_df)
Cancelled
spark.createDataFrame(pd_results_df).write.parquet("dbfs://Cervix Data/params/params1_v11_trainresults.parquet")
Cancelled
import numpy as np
import cv2
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
def scaling(img,scale_x,scale_y):
      """
      Input an image in a numpy array, scale of the x and ydirection as a decimal
      Outputs a numpy array as the scaled image
      """
      return cv2.resize(img,None,fx=scale_x, fy=scale_y, interpolation = cv2.INTER_LINEAR)
def min_max_normalization(x,min,max):
    """
    This function takes an n by m array and normalizes each value based on the average of min and max values
    of the RBG scale (min=0 and max=255).

    It return an n by m array
    """
    avg_value=(max+min)/2.0
    norm_array = np.zeros(x.shape)+avg_value
    normalized_x= (x-norm_array)/norm_array
    return normalized_x

def perspectiveTransform(img_src,offset):
    img = img_src
    cols,rows,ch = img.shape
    pts1 = np.float32([[0,0],[rows,0],[rows,cols],[0,cols]])
    pts2 = np.float32([[np.random.randint(low=0-offset,high=offset,size=1),
                        np.random.randint(low=0-offset,high=offset,size=1)],
                       [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                        np.random.randint(low=0-offset,high=offset,size=1)],
                       [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                        np.random.randint(low=cols-offset,high=cols+offset,size=1)],
                       [np.random.randint(low=0-offset,high=offset,size=1),
                        np.random.randint(low=cols-offset,high=cols+offset,size=1)]])

    M = cv2.getPerspectiveTransform(pts1,pts2)

    dst = cv2.warpPerspective(img,M,(rows,cols),borderMode=cv2.BORDER_REPLICATE)

    return dst

def rotation(img, rotation_angle):
    rows,cols = img.shape[0:2]

    M = cv2.getRotationMatrix2D((cols/2,rows/2),rotation_angle,1)
    return cv2.warpAffine(img,M,(cols,rows))

def image_read(path, flip, aug):
  img = cv2.imread(path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  if flip == 1:
    img = np.fliplr(img)

  if aug==0:
      img = img
  elif aug==1:
      rotation_angle = np.random.randint(low=-60,high=61,size=1)
      img= rotation(img, rotation_angle)
  elif aug==2:
      aug = np.random.randint(low=0,high=35,size=1)
      img=perspectiveTransform(img,35)

  #height_delta = max_height - img.shape[0]
  #width_delta = max_width - img.shape[1]
  return min_max_normalization(img,0,255)
image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")

train, valid = train_test_split(image_data, test_size=0.1, random_state = 100)

train_reg = train.copy()
train_reg["augmentation"] = 0

train_rot = train.copy()
train_rot["augmentation"] = 1

train_warp = train.copy()
train_warp["augmentation"] = 2

train_aug = pd.concat([train_reg,train_rot,train_warp])

train_flip = train_aug.copy()
train_flip["flip"] = 1

train_nonflip = train_aug.copy()
train_nonflip["flip"] = 0

train_final = shuffle(pd.concat([train_nonflip,train_flip]))
valid["flip"] = 0
valid["augmentation"] = 0
/local_disk0/tmp/1551719388564-0/PythonShell.py:22: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  # the backend will not be properly initialized to use AGG.
/local_disk0/tmp/1551719388564-0/PythonShell.py:23: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  import matplotlib as mpl
images = []
angles = []
i =0
for key, batch_sample in train.iterrows():
  if i % 100 == 0:
    print(i)
  name = batch_sample.dst_path
  img = cv2.imread(name)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  #rotation_angle = np.random.randint(low=-60,high=61,size=1)
  #img_rot= rotation(img, rotation_angle)
  #aug = np.random.randint(low=1,high=20,size=1)[0]
  #img_aug=perspectiveTransform(img,aug)
  #img_flip = np.fliplr(img)
  #rotation_angle = np.random.randint(low=-60,high=61,size=1)
  #img_flip_rot= rotation(img_flip, rotation_angle)
  #aug = np.random.randint(low=1,high=20,size=1)[0]
  #img_flip_aug=perspectiveTransform(img_flip,aug)  
  #center_image = image_read(name, batch_sample.flip, batch_sample.augmentation)
  center_angle = [0,0,0]
  center_angle[batch_sample.label] = 1
  images.append(img)
  #images.append(img_rot)
  #images.append(img_aug)
  #images.append(img_flip)
  #images.append(img_flip_rot)
  #images.append(img_flip_aug)
  angles.append(center_angle)
  #angles.append(center_angle)
  #angles.append(center_angle)
  #angles.append(center_angle)
  #angles.append(center_angle)
  #angles.append(center_angle)
  #images.append(np.fliplr(center_image))
  #angles.append([1-center_angle,center_angle])
  i+=1
X_train = np.array(images)
y_train = np.array(angles)
0
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
2100
2200
2300
2400
2500
2600
2700
2800
2900
3000
3100
3200
3300
3400
3500
3600
3700
3800
3900
4000
4100
4200
4300
4400
4500
4600
4700
4800
4900
5000
5100
5200
5300
5400
5500
5600
5700
5800
5900
6000
6100
6200
6300
6400
6500
6600
6700
6800
images = []
angles = []
i=0
for key, batch_sample in valid.iterrows():
  if i % 100 == 0:
    print(i)
  name = batch_sample.dst_path
  img = cv2.imread(name)
  img = min_max_normalization(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), 0 ,255) 
  #center_image = image_read(name, batch_sample.flip, batch_sample.augmentation)
  center_angle = [0,0,0]
  center_angle[batch_sample.label] = 1
  images.append(img)
  angles.append(center_angle)

  #images.append(np.fliplr(center_image))
  #angles.append([1-center_angle,center_angle])
  i+=1
X_val = np.array(images)
y_val = np.array(angles)
0
100
200
300
400
500
600
700
np.save("/dbfs//Cervix Data/same_aspect/X_train.npy", X_train)
np.save("/dbfs//Cervix Data/same_aspect/y_train.npy", y_train)
np.save("/dbfs//Cervix Data/same_aspect/X_val.npy", X_val)
np.save("/dbfs//Cervix Data/same_aspect/y_val.npy", y_val)
X_train = np.load("/dbfs//Cervix Data/same_aspect/X_train.npy")
y_train = np.load("/dbfs//Cervix Data/same_aspect/y_train.npy")
X_val = np.load("/dbfs//Cervix Data/same_aspect/X_val.npy")
y_val = np.load("/dbfs//Cervix Data/same_aspect/y_val.npy")

sc_X_train = sc.broadcast(X_train)
sc_y_train = sc.broadcast(y_train)
sc_X_val = sc.broadcast(X_val)
sc_y_val = sc.broadcast(y_val)
display(train.iloc[0:15])
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/5126.jpg	3264	2448	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/5126.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/5126.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/5126.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_1/3353.jpg	4160	3120	3	16	0	/dbfs/mnt/images/Cervix Data/train_data/train/Type_1/3353.jpg	/dbfs/pepar/Cervix Data/raw/Type_1/3353.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_1/3353.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_3/850.jpg	3264	2448	3	16	2	/dbfs/mnt/images/Cervix Data/train_data/train/Type_3/850.jpg	/dbfs/pepar/Cervix Data/raw/Type_3/850.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_3/850.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/6697.jpg	4128	3096	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/6697.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/6697.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/6697.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/2806.jpg	4128	3096	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/2806.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/2806.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/2806.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/1411.jpg	4128	3096	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/1411.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/1411.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/1411.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_3/4983.jpg	3264	2448	3	16	2	/dbfs/mnt/images/Cervix Data/train_data/train/Type_3/4983.jpg	/dbfs/pepar/Cervix Data/raw/Type_3/4983.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_3/4983.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/234.jpg	4128	3096	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/234.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/234.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/234.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/4273.jpg	3264	2448	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/4273.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/4273.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/4273.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_3/1458.jpg	3264	2448	3	16	2	/dbfs/mnt/images/Cervix Data/train_data/train/Type_3/1458.jpg	/dbfs/pepar/Cervix Data/raw/Type_3/1458.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_3/1458.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_3/1009.jpg	3264	2448	3	16	2	/dbfs/mnt/images/Cervix Data/train_data/train/Type_3/1009.jpg	/dbfs/pepar/Cervix Data/raw/Type_3/1009.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_3/1009.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_1/3479.jpg	4160	3120	3	16	0	/dbfs/mnt/images/Cervix Data/train_data/train/Type_1/3479.jpg	/dbfs/pepar/Cervix Data/raw/Type_1/3479.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_1/3479.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/2946.jpg	4128	3096	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/2946.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/2946.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/2946.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/77.jpg	4128	3096	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/77.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/77.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/77.jpg
dbfs:/mnt/images/Cervix Data/train_data/train/Type_2/471.jpg	4128	3096	3	16	1	/dbfs/mnt/images/Cervix Data/train_data/train/Type_2/471.jpg	/dbfs/pepar/Cervix Data/raw/Type_2/471.jpg	/dbfs/pepar/Cervix Data/same_aspect/Type_2/471.jpg
origin	height	width	nChannels	mode	label	src_path_datalake	src_path_dbfs	dst_path
