databricks-logo5.7. Training Best of Multiple Models for Ensemble - Broadcasting Images(Python) Import Notebook
Image Processing

Distributed Deep Learning

Get a reference dataframe of location to save model, and parameters of model training
Define function to train model with given parameters and save model to location as specified
Apply .rdd.map to reference dataframe (be sure the partition number is higher than 1)
#read only colored images in
X_train = np.load("/dbfs//Cervix Data/same_aspect/X_train.npy")
y_train = np.load("/dbfs//Cervix Data/same_aspect/y_train.npy")
X_val = np.load("/dbfs//Cervix Data/same_aspect/X_val.npy")
y_val = np.load("/dbfs//Cervix Data/same_aspect/y_val.npy")
#X_train_gray = np.load("/dbfs//Cervix Data/same_aspect/X_train_gray.npy")
#y_train_gray = np.load("/dbfs//Cervix Data/same_aspect/y_train_gray.npy")
#X_val_gray = np.load("/dbfs//Cervix Data/same_aspect/X_val_gray.npy")
#y_val_gray = np.load("/dbfs//Cervix Data/same_aspect/y_val_gray.npy")
#broadcast numpy objects
sc_X_train = sc.broadcast(X_train)
sc_y_train = sc.broadcast(y_train)
sc_X_val = sc.broadcast(X_val)
sc_y_val = sc.broadcast(y_val)
#sc_X_train_gray = sc.broadcast(X_train_gray)
#sc_y_train_gray = sc.broadcast(y_train_gray)
#sc_X_val_gray = sc.broadcast(X_val_gray)
#sc_y_val_gray = sc.broadcast(y_val_gray)
#read in csv created of refined models to continue training
from pyspark.sql.functions import *
from pyspark.sql.types import *
customSchema = StructType(
        [StructField("ModelLocation", StringType(), False),
        StructField("Epochs", IntegerType(), False),
          StructField("ColorType", StringType(), False),
         StructField("Type", StringType(), False),
          StructField("Alpha", DoubleType(), False),
        
       
        StructField("Iters", IntegerType(), False),
        StructField("EpochsSize", IntegerType(), False)])
#row = models_df_tune.toPandas()
#row["Epochs"] = 30
models_df_tune=spark.read.format("csv").schema(customSchema).option("header","true").load("dbfs:/FileStore/tables/refined.csv")
display(models_df_tune)
/dbfs/pepar/Cervix Data/old_model/alpha_model_4_Class2_Color_weights.h5	20	Color	Class2	0	4	5
/dbfs/pepar/Cervix Data/old_model/alpha_model_3_Class1_Color_weights.h5	20	Color	Class1	0	4	5
/dbfs/pepar/Cervix Data/old_model/alpha_model_7_Class3_Color_weights.h5	20	Color	Class3	0	4	5
/dbfs/pepar/Cervix Data/old_model/alpha_model11_batchnorm_Color_Weights.h5	0	Color	Normal	0.000490559	4	10
/dbfs/pepar/Cervix Data/old_model/alpha_model_6_Class1_Color_weights.h5	20	Color	Class1	0	4	5
/dbfs/pepar/Cervix Data/old_model/alpha_model_2_Class1_Color_weights.h5	20	Color	Class1	0	4	5
/dbfs/pepar/Cervix Data/old_model/alpha_model_9_Class3_Color_weights.h5	20	Color	Class3	0	4	5
/dbfs/pepar/Cervix Data/old_model/alpha_model_2_Class3_Color_weights.h5	20	Color	Class3	0	4	5
/dbfs/pepar/Cervix Data/old_model/alpha_model_3_Class2_Color_weights.h5	20	Color	Class2	0	4	5
ModelLocation	Epochs	ColorType	Type	Alpha	Iters	EpochsSize
 def runDLModelHyper(row):
  import pandas as pd
  #spark_image_data = spark.read.parquet("dbfs://Cervix Data/same_aspect/same_aspect/processed_image_metadata.parquet")
  #image_data = pd.read_json("/dbfs//Cervix Data/params/params1.json")
  from sklearn.model_selection import train_test_split
  from keras.callbacks import ModelCheckpoint
  from sklearn.utils import shuffle
  import numpy as np
  from keras.layers.advanced_activations import ELU
  import os
  from keras.models import Sequential
  from keras.layers.core import Dense, Activation, Flatten, Dropout
  from keras.layers.convolutional import Convolution2D
  from keras.layers.pooling import MaxPooling2D
  #from keras.layers.advanced_activations import ELU
  #import pandas as pd
  import datetime
  from keras.layers.normalization import BatchNormalization
  #import matplotlib.image as mpimg
  import numpy as np
  import time
  import datetime
  import json
  import os
  import tensorflow as tf
  #import horovod.tensorflow
  from tensorflow.contrib import learn
  from keras.datasets import mnist
  #import horovod.tensorflow.keras as hvd
  from tensorflow import keras
  from tensorflow.keras import backend as K
  from tensorflow.keras import layers
  from tensorflow.keras.datasets import mnist
  #from sparkdl import HorovodRunner
  import keras
  #from sklearn.utils import shuffle
  import numpy as np
  import cv2
  from tensorflow.python.client import device_lib
  device_lib.list_local_devices()
  from keras.utils import multi_gpu_model
  from keras import backend as K
  K.tensorflow_backend._get_available_gpus()
  
  def min_max_normalization(x,min,max):
    """
    This function takes an n by m array and normalizes each value based on the average of min and max values
    of the RBG scale (min=0 and max=255).

    It return an n by m array
    """
    avg_value=(max+min)/2.0
    norm_array = np.zeros(x.shape)+avg_value
    normalized_x= (x-norm_array)/norm_array
    return normalized_x
  def perspectiveTransform(img_src,offset):
      img = img_src
      if len(img.shape) == 3:
        cols,rows,ch = img.shape
      else: 
        cols,rows = img.shape
      pts1 = np.float32([[0,0],[rows,0],[rows,cols],[0,cols]])
      pts2 = np.float32([[np.random.randint(low=0-offset,high=offset,size=1),
                          np.random.randint(low=0-offset,high=offset,size=1)],
                         [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                          np.random.randint(low=0-offset,high=offset,size=1)],
                         [np.random.randint(low=rows-offset,high=rows+offset,size=1),
                          np.random.randint(low=cols-offset,high=cols+offset,size=1)],
                         [np.random.randint(low=0-offset,high=offset,size=1),
                          np.random.randint(low=cols-offset,high=cols+offset,size=1)]])

      M = cv2.getPerspectiveTransform(pts1,pts2)

      dst = cv2.warpPerspective(img,M,(rows,cols),borderMode=cv2.BORDER_REPLICATE)

      return dst

  def rotation(img, rotation_angle):
      rows,cols = img.shape[0:2]

      M = cv2.getRotationMatrix2D((cols/2,rows/2),rotation_angle,1)
      return cv2.warpAffine(img,M,(cols,rows))
  
  #train, valid = train_test_split(image_data, test_size=0.1)
  
  
  #from keras.models import load_model
  #model = load_model(str(row.ModelLocation)[:-7]+"_v11.h5")
  if int(row.Epochs) == 0:
    print("Build new model")
    model = Sequential()
    elu=ELU()
    #model.add(Convolution2D(5, 5, 5, batch_input_shape=(None, 450,600,3)))
    if row.ColorType == "Color":
      model.add(Convolution2D(5, (5, 5), batch_input_shape=(None, 200,150,3)))
    else:
      model.add(Convolution2D(5, (5, 5), batch_input_shape=(None, 200,150,1)))
    model.add(BatchNormalization())
    model.add(elu)
    model.add(MaxPooling2D((2, 1)))
    model.add(Dropout(0.3))

    model.add(Convolution2D(7, (5, 5)))
    model.add(BatchNormalization())
    model.add(elu)
    model.add(Dropout(0.3))

    model.add(Convolution2D(9, (5, 5)))
    model.add(BatchNormalization())
    model.add(elu)
    model.add(MaxPooling2D((1, 2)))
    model.add(Dropout(0.3))


    model.add(Convolution2D(10, (3, 3)))
    model.add(BatchNormalization())
    model.add(elu)
    model.add(MaxPooling2D((2, 1)))
    model.add(Dropout(0.3))


    model.add(Convolution2D(15, (3, 3)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((1, 2)))
    model.add(elu)
    model.add(Dropout(0.3))

    model.add(Convolution2D(18, (2, 2)))
    model.add(BatchNormalization())
    model.add(elu)
    #model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.3))


    model.add(Convolution2D(30, (2, 2)))
    model.add(BatchNormalization())
    model.add(elu)
    #model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.3))



    model.add(Flatten())
    model.add(Dense(1000))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(800))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(700))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(600))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(500))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(400))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(300))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(200))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))

    model.add(Dense(100))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    model.add(Dense(50))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    """model.add(Dense(40))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))

    model.add(Dense(30))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))

    model.add(Dense(25))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))

    model.add(Dense(20))
    model.add(Activation('relu'))
    #model.add(Dropout(0.2))"""

    model.add(Dense(15))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    #model.add(Dense(5))
    #model.add(Activation('relu'))

    if row.Type == "Normal":
      model.add(Dense(3))
    else:
      model.add(Dense(2))
    model.add(Activation('softmax'))
  else:
    print("Load existing model")
    from keras.models import load_model
    model = load_model(str(row.ModelLocation))

  model.compile(loss=keras.losses.categorical_crossentropy,
                optimizer=keras.optimizers.Adam(lr = row.Alpha),
                metrics=['accuracy'])
  filepath="/dbfs//Cervix Data/old_model/checkpoints/"+str(row.ModelLocation).split("/")[-1][:-3]+"_correct-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}.h5"
  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0)
  checkpoint.epochs_since_last_save  = int(row.Epochs)
  #batch_size = 100
  from sklearn.utils import class_weight

  iters =int(row.Iters)
  epochs=int(row.EpochsSize)
  #csv_logger = CSVLogger("/dbfs//Cervix Data/old_model/history/"+str(row.ModelLocation).split("/")[-1][:-3]+"_noclassweights_extradropout.csv", append=True, separator=',')
  #callbacks_list = [csv_logger, checkpoint]
  
  #model.fit(X_train, y_train, batch_size = 100, epochs=epochs, class_weight = class_weights, verbose=1)
  from keras.callbacks import CSVLogger

  csv_logger = CSVLogger("/dbfs//Cervix Data/old_model/history/"+str(row.ModelLocation).split("/")[-1][:-3]+"_correct.csv", append=True, separator=',')
  callbacks_list = [csv_logger, checkpoint]
  if row.ColorType == "Color":
    temp_x_train = sc_X_train.value
    temp_y_train = sc_y_train.value
    temp_y_val = sc_y_val.value
  else:
    temp_x_train = sc_X_train_gray.value
    temp_y_train = sc_y_train_gray.value
    temp_y_val = sc_y_val_gray.value
  
  for iter_i in range(0,iters):
    print("Iter " + str(iter_i),str(row.ModelLocation)[:-3]+"_weights.h5")
    images = []
    angles = []
    class_list = []
    print("augmenting Data")
    for batch_sample, label in zip(temp_x_train,temp_y_train):
      
      X_train = None
      y_train = None
      #name = batch_sample.dst_path
      if row.ColorType == "Color":
        height, width, channel = batch_sample.shape
        img = batch_sample
      else:
        height, width = batch_sample.shape
        channel = 1
        img = np.resize(batch_sample, (height, width, 1))
      
      #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
      rotation_angle = np.random.randint(low=-60,high=61,size=1)
      img_rot= np.resize(min_max_normalization(rotation(img, rotation_angle), 0, 255), (height, width, channel))
      #aug = np.random.randint(low=1,high=20,size=1)[0]
      img_aug=np.resize(min_max_normalization(perspectiveTransform(img,15), 0, 255), (height, width, channel))
      img_flip = np.resize(min_max_normalization(np.fliplr(img), 0, 255), (height, width, channel))
      rotation_angle = np.random.randint(low=-60,high=61,size=1)
      img_flip_rot= np.resize(min_max_normalization(rotation(img_flip, rotation_angle), 0, 255), (height, width, channel))
      #aug = np.random.randint(low=1,high=20,size=1)[0]
      img_flip_aug=np.resize(min_max_normalization(perspectiveTransform(img_flip,15), 0, 255) , (height, width, channel))
      img = min_max_normalization(img, 0, 255)
      #center_image = image_read(name, batch_sample.flip, batch_sample.augmentation)
      center_angle = []
      if row.Type == "Class1":
        center_angle.append(label[0])
        center_angle.append(label[1]+label[2])
        
        if label[0] == 1:
          class_list.append(0)
        else:
          class_list.append(1)
      elif row.Type == "Class2":
        center_angle.append(label[1])
        center_angle.append(label[0]+label[2])
        if label[1] == 1:
          class_list.append(0)
        else:
          class_list.append(1)
      elif row.Type == "Normal":
        center_angle = label
        if label[0] == 1:
          class_list.append(0)
        elif label[1] == 1:
          class_list.append(1)
        else:
          class_list.append(2)
      else:
        center_angle.append(label[2])
        center_angle.append(label[0]+label[1])
        if label[2] == 1:
          class_list.append(0)
        else:
          class_list.append(1)
      #center_angle[batch_sample.label] = 1
      images.append(img)
      images.append(img_rot)
      images.append(img_aug)
      images.append(img_flip)
      images.append(img_flip_rot)
      images.append(img_flip_aug)
      angles.append(center_angle)
      angles.append(center_angle)
      angles.append(center_angle)
      angles.append(center_angle)
      angles.append(center_angle)
      angles.append(center_angle)
      #images.append(np.fliplr(center_image))
      #angles.append([1-center_angle,center_angle])
    temp = []
    for label in temp_y_val:
      
      y_val = None

      y_angle = []
      if row.Type == "Class1":
        y_angle.append(label[0])
        y_angle.append(label[1]+label[2])
      elif row.Type == "Class2":
        y_angle.append(label[1])
        y_angle.append(label[0]+label[2])
      elif row.Type == "Normal":
        y_angle = label
      else:
        y_angle.append(label[2])
        y_angle.append(label[0]+label[1])
      #center_angle[batch_sample.label] = 1

      temp.append(y_angle)
      #images.append(np.fliplr(center_image))
      #angles.append([1-center_angle,center_angle])
    y_val = np.array(temp)
    X_train = np.array(images)
    y_train = np.array(angles)
    X_train, y_train = shuffle(X_train, y_train)
    print(sc_X_train.value.shape, X_train.shape)
    class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(class_list),
                                                 class_list)
    if row.ColorType == "Color":
      model.fit(X_train, y_train, batch_size = 100, epochs=row.Epochs+(epochs*iter_i)+epochs, verbose=1,callbacks=callbacks_list, validation_data=(sc_X_val.value, y_val), initial_epoch=row.Epochs+(epochs*iter_i), class_weight = class_weights)
    else:
      model.fit(X_train, y_train, batch_size = 100, epochs=row.Epochs+(epochs*iter_i)+epochs, verbose=0,callbacks=callbacks_list, validation_data=(np.resize(sc_X_val.value, (763,200,150,1)), y_val), initial_epoch=row.Epochs+(epochs*iter_i), class_weight = class_weights)
    
    
  model.save(str(row.ModelLocation)[:-3]+"_weights.h5")
  
  '''if row.ColorType == "Color":
    score = model.evaluate(sc_X_val.value, y_val,verbose=0)
  else:
    score = model.evaluate(np.resize(sc_X_val.value, (763,200,150,1)), y_val,verbose=0)
  acc = float(score[1])
  loss = float(score[0])'''
  acc = -1.0
  loss = -1.0
  return(str(row.ModelLocation), row.Alpha,  loss, acc, row.Epochs+(epochs*iters))
schema = StructType([
  StructField("ModelLocation", StringType(), False),
  StructField("Alpha", DoubleType(), False),
  StructField("Loss", DoubleType(), False),
  StructField("Accuracy", DoubleType(), False),
   StructField("Epochs", IntegerType(), False)
                    ])
models_df_tune = models_df_tune.repartition(9)
models_df_tune.cache()
Out[10]: DataFrame[ModelLocation: string, Epochs: int, ColorType: string, Type: string, Alpha: double, Iters: int, EpochsSize: int]
#run distributed models
results_df = models_df_tune.rdd.map(runDLModelHyper).toDF(schema).toPandas()
